{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Machine Learning Model\n",
    "#### Created by Randhir and Andrew\n",
    "\n",
    "Model that will take a $90\\times120$ thumbnail JPEG and title from YouTube to output a video performance metric.\n",
    "The metric will be \n",
    "$$Score=\\frac{Amount\\ of\\ Likes}{Amount\\ of\\ Views}$$\n",
    "The idea is how many people have seen this video and decided it desires a like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re, os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, TextVectorization, Embedding, Dropout, Concatenate, Input\n",
    "from keras import Model\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# File Structure\n",
    "dirpath = \"thumbnail\"\n",
    "datafile = \"data-filtered.csv\"\n",
    "\n",
    "# Regex Patterns\n",
    "emoji_re = \"[\\U000000A9-\\U0010ffff]\"\n",
    "punc_re = f\"[{re.escape(string.punctuation)}]\"\n",
    "\n",
    "# Download Stopwords & pattern\n",
    "nltk.download('stopwords')\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "sw_re = f'(?:{\"|\".join([f\"{re.escape(sw)}\" for sw in stopwords_list])})'\n",
    "\n",
    "# Text Model Settings\n",
    "max_features = 20000\n",
    "embedding_dim = 128\n",
    "sequence_length = 500\n",
    "\n",
    "# KFold Settings\n",
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing\n",
    "This process involves text processing and image processing. This will involve text standardization and vectorization. For the image, it needs to be processed and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thumbnail-w</th>\n",
       "      <th>thumbnail-h</th>\n",
       "      <th>view-count</th>\n",
       "      <th>like-count</th>\n",
       "      <th>comment-count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35757.0</td>\n",
       "      <td>35757.0</td>\n",
       "      <td>3.529900e+04</td>\n",
       "      <td>3.431000e+04</td>\n",
       "      <td>35026.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>9.281597e+04</td>\n",
       "      <td>2.302683e+03</td>\n",
       "      <td>115.941187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.687363e+06</td>\n",
       "      <td>3.116437e+04</td>\n",
       "      <td>1640.474869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3.000000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.820000e+02</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3.154500e+03</td>\n",
       "      <td>7.500000e+01</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.232996e+08</td>\n",
       "      <td>2.686147e+06</td>\n",
       "      <td>146332.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       thumbnail-w  thumbnail-h    view-count    like-count  comment-count\n",
       "count      35757.0      35757.0  3.529900e+04  3.431000e+04   35026.000000\n",
       "mean         120.0         90.0  9.281597e+04  2.302683e+03     115.941187\n",
       "std            0.0          0.0  1.687363e+06  3.116437e+04    1640.474869\n",
       "min          120.0         90.0  0.000000e+00  0.000000e+00       0.000000\n",
       "25%          120.0         90.0  3.000000e+01  1.000000e+00       0.000000\n",
       "50%          120.0         90.0  2.820000e+02  8.000000e+00       1.000000\n",
       "75%          120.0         90.0  3.154500e+03  7.500000e+01      12.000000\n",
       "max          120.0         90.0  2.232996e+08  2.686147e+06  146332.000000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(datafile, index_col=\"yt-id\")\n",
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Processing\n",
    "def text_standardization(raw_strs):\n",
    "\tlower = tf.strings.lower(raw_strs)\n",
    "\temojiless = tf.strings.regex_replace(lower, emoji_re, \"\")\n",
    "\tstopwrdless = tf.strings.regex_replace(emojiless, sw_re, \"\")\n",
    "\tpunctuationless = tf.strings.regex_replace(stopwrdless, punc_re, \"\")\n",
    "\treturn punctuationless\n",
    "\n",
    "# Vectorization Layer\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=text_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Processing\n",
    "files = [f for f in os.listdir(dirpath) if os.path.isfile(f\"{dirpath}/{f}\") and f.endswith(\".jpg\")]\n",
    "images = np.zeros((len(files), 90, 120, 3))\n",
    "image_ids = []\n",
    "i = 0\n",
    "for f in files:\n",
    "\ttry:\n",
    "\t\tim = Image.open(f\"{dirpath}/{f}\")\n",
    "\t\timages[i] = np.array(im)\n",
    "\t\timage_ids.append(f[:-4])\n",
    "\t\tim.close()\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\n",
    "\ti += 1\n",
    "\n",
    "# Normalize Pixels\n",
    "images /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    35734.000000\n",
       "mean         0.306272\n",
       "std          0.172596\n",
       "min          0.000000\n",
       "25%          0.173335\n",
       "50%          0.290248\n",
       "75%          0.416793\n",
       "max          1.000000\n",
       "Name: view-count, dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Processing\n",
    "scores = raw_data[\"view-count\"] # Grab View Count\n",
    "scores = scores[image_ids] # Filter for images that we have\n",
    "scores = scores.fillna(0.0) # Replace NaN with 0\n",
    "scores = scores.map(lambda x : np.log10(x + 1)) # Log everything to make it less extreme\n",
    "scores /= scores.max() # Normalized\n",
    "\n",
    "scores.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly, the Sequential API is used to train a model. However, due to the need for more than one input, the Functional API must be employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"img_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 90, 120, 3)]      0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 88, 118, 16)       448       \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPooli  (None, 44, 59, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 42, 57, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPooli  (None, 21, 28, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 19, 26, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPooli  (None, 9, 13, 64)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 7488)              0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 100)               748900    \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 772585 (2.95 MB)\n",
      "Trainable params: 772585 (2.95 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Image Portion\n",
    "img_input = Input((90, 120, 3))\n",
    "x = Conv2D(16, 3, activation='relu', kernel_initializer='he_uniform')(img_input)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(32, 3, activation='relu', kernel_initializer='he_uniform')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(64, 3, activation='relu', kernel_initializer='he_uniform')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(100, activation='relu', kernel_initializer='he_uniform')(x)\n",
    "img_output = Dense(1, activation='softmax')(x)\n",
    "\n",
    "img_model = Model(inputs=img_input, outputs=img_output, name=\"img_model\")\n",
    "\n",
    "img_model.summary()\n",
    "img_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 1, 128)            2560000   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1, 128)            0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1, 100)            12900     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1, 1)              101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2573001 (9.82 MB)\n",
      "Trainable params: 2573001 (9.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Text Portion\n",
    "text_input = Input((1,), dtype=tf.string)\n",
    "y = Embedding(max_features, embedding_dim)(text_input)\n",
    "y = Dropout(0.5)(y)\n",
    "y = Dense(100, activation='relu', kernel_initializer='he_uniform')(y)\n",
    "text_output = Dense(1, activation='softmax')(y)\n",
    "\n",
    "text_model = Model(inputs=text_input, outputs=text_output, name=\"text_model\")\n",
    "\n",
    "text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 100), (None, 1, 100)]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# United Model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mConcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m z \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m10\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, kernel_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhe_uniform\u001b[39m\u001b[38;5;124m'\u001b[39m)(z)\n\u001b[0;32m      4\u001b[0m z \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)(z)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\merging\\concatenate.py:119\u001b[0m, in \u001b[0;36mConcatenate.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    117\u001b[0m ranks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mlen\u001b[39m(shape) \u001b[38;5;28;01mfor\u001b[39;00m shape \u001b[38;5;129;01min\u001b[39;00m shape_set)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ranks) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Get the only rank for the set.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m (rank,) \u001b[38;5;241m=\u001b[39m ranks\n",
      "\u001b[1;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 100), (None, 1, 100)]"
     ]
    }
   ],
   "source": [
    "# United Model\n",
    "z = Concatenate()([x, y])\n",
    "z = Dense(10, activation='relu', kernel_initializer='he_uniform')(z)\n",
    "z = Dense(1, activation='softmax')(z)\n",
    "\n",
    "united_model = Model(inputs=[img_input, text_input], outputs=z, name=\"unitied_model\")\n",
    "\n",
    "united_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Using k-fold cross validation, we can judge the accuarcy of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [35757, 35734]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m validation_accuracy \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m validation_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m\t\u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:406\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m        The testing set indices for that split.\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m     X, y, groups \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:514\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 514\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [35757, 35734]"
     ]
    }
   ],
   "source": [
    "# Image Model\n",
    "kf = KFold(n_folds)\n",
    "\n",
    "validation_accuracy = []\n",
    "validation_loss = []\n",
    "\n",
    "for train, val in kf.split(images, scores):\n",
    "\tpass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
