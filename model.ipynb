{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Machine Learning Model\n",
    "#### Created by Randhir and Andrew\n",
    "\n",
    "Model that will take a $90\\times120$ thumbnail JPEG and title from YouTube to output a video performance metric.\n",
    "The metric will be \n",
    "$$Score=\\log{(View\\ Count + 1)}$$\n",
    "The idea is that the video that attracted more views is a good video. The value is log-scaled as the higher the view count, the less meaningful it becomes. This value will be normalized with the maximum value in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re, requests, os, json, random\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "from pathlib import Path\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_keras.backend import clear_session\n",
    "from tf_keras.callbacks import ModelCheckpoint\n",
    "from tf_keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Concatenate, TextVectorization, Input\n",
    "from tf_keras.preprocessing.sequence import pad_sequences\n",
    "from tf_keras.utils import Sequence\n",
    "from tf_keras import Model\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file with your api key\n",
    "if not load_dotenv():\n",
    "\tprint(\".env file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants\n",
    "This cell contains the constants used by this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# File Structure\n",
    "dirpath = \"thumbnail\"\n",
    "modeldir = \"models\"\n",
    "datafile = \"data-filtered.csv\"\n",
    "\n",
    "# Data Aquisition\n",
    "filepath = \"data.csv\"\n",
    "count = 50\n",
    "max_iterations = 100 # 50 * 100 = 5000 videos\n",
    "topic_id = \"/m/03hf_rm\" # Strategy Games\n",
    "lang = \"en\"\n",
    "API_KEY = os.getenv(\"APIKEY\")\n",
    "\n",
    "# Data Filtering\n",
    "MULT_CSV = False\n",
    "filepath = \"data.csv\"\n",
    "filepath_2 = \"data_2.csv\"\n",
    "filepath_final = \"data-filtered.csv\"\n",
    "lang = \"en\"\n",
    "\n",
    "# Labeling\n",
    "MAX_VIEWS = 15.3e9 # Baby Shark Video (Most Viewed Video)\n",
    "THRESHOLD = 0.4\n",
    "\n",
    "# Vectorization\n",
    "vectorizator_model = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(vectorizator_model)\n",
    "transformer_model = TFAutoModel.from_pretrained(vectorizator_model)\n",
    "\n",
    "# Regex Patterns\n",
    "emoji_re = \"[\\U000000A9-\\U0010ffff]\"\n",
    "punc_re = f\"[{re.escape(string.punctuation)}]\"\n",
    "space_re = \"\\s{1,}\"\n",
    "\n",
    "# Download Stopwords & pattern\n",
    "nltk.download('stopwords')\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "sw_re = f'\\b(?:{\"|\".join([f\"{re.escape(sw)}\" for sw in stopwords_list])})\\b'\n",
    "\n",
    "# Text Model Settings\n",
    "text_input_dim = 20000\n",
    "embedding_dim = 128\n",
    "sequence_length = 500\n",
    "\n",
    "# KFold Settings\n",
    "n_folds = 5\n",
    "epochs = 10\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Aquisition\n",
    "The YouTube API is used to get video data. This includes a video's thumbnail and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data file already exist\n",
    "if os.path.isfile(filepath):\n",
    "    df = pd.read_csv(filepath, index_col=\"yt-id\")\n",
    "else:\n",
    "    df = pd.DataFrame([], columns=[\"yt-id\", \"title\", \"created\", \"channel-id\", \"thumbnail\", \"thumbnail-w\", \"thumbnail-h\", \"view-count\", \"like-count\", \"comment-count\", \"query\"])\n",
    "    df = df.set_index(\"yt-id\")\n",
    "    \n",
    "# Grab missing data IDs for query\n",
    "yt_ids = list(df[df[\"view-count\"].isna()].index)\n",
    "\n",
    "# Loop\n",
    "yt_reads = 0\n",
    "for i in range(max_iterations):\n",
    "    try:\n",
    "        # Check if any stats calls are needed\n",
    "        if len(yt_ids) > 0:\n",
    "            # Message \n",
    "            print(\"Pulling statistics for missing data values\")\n",
    "\n",
    "            # Split up batch by 50 if needed\n",
    "            for index_split in range(50, len(yt_ids) + 1, 50):\n",
    "                # Generate & call statistic query (1 unit)\n",
    "                urlData_stats = f\"https://www.googleapis.com/youtube/v3/videos?key={API_KEY}&part=statistics&id={','.join(yt_ids[index_split - 50:index_split])}\"\n",
    "                webURL_stats = urllib.request.urlopen(urlData_stats)\n",
    "                raw_stats_data = webURL_stats.read()\n",
    "                results_stats = json.loads(raw_stats_data.decode(webURL_stats.info().get_content_charset('utf-8')))\n",
    "\n",
    "                # Process Stats Response\n",
    "                for stats_data in results_stats[\"items\"]:\n",
    "                    try:\n",
    "                        # Parse data\n",
    "                        new_row = pd.DataFrame([{\n",
    "                            \"yt-id\": stats_data['id'],\n",
    "                            \"view-count\": stats_data['statistics']['viewCount'],\n",
    "                            \"like-count\": stats_data['statistics']['likeCount'] if 'likeCount' in stats_data['statistics'] else \"\",\n",
    "                            \"comment-count\": stats_data['statistics']['commentCount'] if 'commentCount' in stats_data['statistics'] else \"\",\n",
    "                        },])\n",
    "                        new_row = new_row.set_index(\"yt-id\")\n",
    "\n",
    "                        # Update main dataset\n",
    "                        df.update(new_row)\n",
    "                    except KeyError:\n",
    "                        # Weird Entry\n",
    "                        continue\n",
    "\n",
    "            # Reset after used\n",
    "            yt_ids = [] \n",
    "\n",
    "            # Message \n",
    "            print(\"Finished pulling statistics for current batch\")\n",
    "\n",
    "        # Message\n",
    "        print(f\"Pulling {count} random videos\")\n",
    "\n",
    "        # Generates random query for YT\n",
    "        r_q = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(3))\n",
    "\n",
    "        # Calls the API for search results (100 units)\n",
    "        urlData_query = f\"https://www.googleapis.com/youtube/v3/search?key={API_KEY}&maxResults={count}&part=snippet&type=video&relevanceLanguage={lang}&topicId={topic_id}&q={r_q}\"\n",
    "        webURL_query = urllib.request.urlopen(urlData_query)\n",
    "        raw_vid_data = webURL_query.read()\n",
    "        results_vids = json.loads(raw_vid_data.decode(webURL_query.info().get_content_charset('utf-8')))\n",
    "\n",
    "        # Process Video Response\n",
    "        for video_data in results_vids['items']:\n",
    "            # Ignore Live and Upcoming Content (no ratings yet)\n",
    "            if video_data['snippet']['liveBroadcastContent'] != \"none\":\n",
    "                continue\n",
    "\n",
    "            # Parse data\n",
    "            try:\n",
    "                new_row = pd.DataFrame([{\n",
    "                    \"yt-id\": video_data['id']['videoId'],\n",
    "                    \"title\": video_data['snippet']['title'],\n",
    "                    \"created\": video_data['snippet']['publishedAt'],\n",
    "                    \"channel-id\": video_data['snippet']['channelId'],\n",
    "                    \"thumbnail\": video_data['snippet']['thumbnails'][\"default\"][\"url\"],\n",
    "                    \"thumbnail-w\": video_data['snippet']['thumbnails'][\"default\"][\"width\"],\n",
    "                    \"thumbnail-h\": video_data['snippet']['thumbnails'][\"default\"][\"height\"],\n",
    "                    \"query\": r_q,\n",
    "                },])\n",
    "                new_row = new_row.set_index(\"yt-id\")\n",
    "\n",
    "                try:\n",
    "                    # Append\n",
    "                    df = pd.concat([df, new_row], verify_integrity=True)\n",
    "\n",
    "                    # Store your ids\n",
    "                    yt_reads += 1\n",
    "\n",
    "                    # Prepare id for stats query\n",
    "                    yt_ids.append(video_data['id']['videoId'])\n",
    "                except ValueError:\n",
    "                    # Duplicate video detected\n",
    "                    continue\n",
    "            except KeyError:\n",
    "                # Weird Entry\n",
    "                continue\n",
    "\n",
    "        # Update User\n",
    "        print(f\"API call #{i + 1} successfully\")\n",
    "\n",
    "        # Dumb Data to prevent loss every 5 runs\n",
    "        if i % 5 == 0:\n",
    "            df.to_csv(filepath)\n",
    "\n",
    "    # ON API failure, quit and save\n",
    "    except urllib.error.HTTPError:\n",
    "        print(\"Latest API call failed. You are likely out of units. Try again tomorrow.\")\n",
    "        break\n",
    "    \n",
    "# Write to csv\n",
    "df.to_csv(filepath)\n",
    "\n",
    "# Termination\n",
    "print(f\"Was able to pull {yt_reads} rows\")\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After aquiring the data, the thumbnail images need to be pulled as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Filtering\n",
    "Some of the pulled data need to be filtered before usage. This includes potential duplicates and non english entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "df = pd.read_csv(filepath, index_col=\"yt-id\")\n",
    "\n",
    "# Merge multiple if needed\n",
    "if MULT_CSV:\n",
    "\tdf_2 = pd.read_csv(filepath_2, index_col=\"yt-id\")\n",
    "\tdf = pd.concat([df, df_2])\n",
    "\n",
    "print(f\"{df.size} rows in data file\")\n",
    "\n",
    "# Remove duplicates\n",
    "df = df[~df.index.duplicated(keep='first')]\n",
    "print(f\"{df.size} rows remaining after duplication filter\")\n",
    "\n",
    "# Remove non language\n",
    "def lang_filter(row) -> bool:\n",
    "\ttry:\n",
    "\t\tprint(row[\"title\"])\n",
    "\t\treturn detect(row[\"title\"]) == lang\n",
    "\texcept LangDetectException:\n",
    "\t\treturn False\n",
    "\t\n",
    "df = df[df.apply(lang_filter, axis=1)]\n",
    "print(f\"{df.size} rows remaining after translation filter\")\n",
    "\n",
    "# Save Filtered Data\n",
    "df.to_csv(filepath_final)\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thumbnail Requesting\n",
    "Once the dataset has been filtered, the thumbnails can now be pulled. Images that do not fit the $90\\times120$ size will be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab data\n",
    "df = pd.read_csv(filepath, index_col=\"yt-id\")\n",
    "\n",
    "# Make directory for image if not already\n",
    "if not os.path.isdir(dirpath):\n",
    "\tos.mkdir(dirpath)\n",
    "\n",
    "# Iterate thru dataframe and download\n",
    "def grab_thumbnail(x : pd.Series):\n",
    "\t# Check if file exist\n",
    "\tfilename = f'{dirpath}/{x.name}.jpg'\n",
    "\tif os.path.isfile(filename):\n",
    "\t\tprint(f\"Thumbnail already retrieved for {x.name}\")\n",
    "\t\treturn\n",
    "\n",
    "\t# Call file\n",
    "\twith open(filename, 'wb') as handle:\n",
    "\t\tprint(f\"Retrieving thumbnail for {x.name}\")\n",
    "\t\tresponse = requests.get(x[\"thumbnail\"], stream=True)\n",
    "\n",
    "\t\t# Fail request\n",
    "\t\tif not response.ok:\n",
    "\t\t\tprint(f\"Could not retrieve thumbnail for {x.name}\")\n",
    "\n",
    "\t\t# Success save\n",
    "\t\tfor block in response.iter_content(1024):\n",
    "\t\t\tif not block:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\thandle.write(block)\n",
    "\n",
    "# Apply to all\n",
    "df.apply(grab_thumbnail, axis=1)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02v-CVttnS0.jpg deleted\n",
      "0EZUP5Vtemw.jpg deleted\n",
      "4KlB4i4dEWU.jpg deleted\n",
      "6JhUQpe-J6U.jpg deleted\n",
      "bAHQy0QFUMI.jpg deleted\n",
      "DcejDtVA4MU.jpg deleted\n",
      "E0Hchyxwr4c.jpg deleted\n",
      "ffLdLgSbpEc.jpg deleted\n",
      "hstJLLvhYSM.jpg deleted\n",
      "htE2M7shdfI.jpg deleted\n",
      "JTwsU2dDpEg.jpg deleted\n",
      "Lpnw6hMIu24.jpg deleted\n",
      "Q9D-aQzRuU4.jpg deleted\n",
      "RPoQZ_926hQ.jpg deleted\n",
      "Sl2ueV8kRRU.jpg deleted\n",
      "StkNJFSGksg.jpg deleted\n",
      "tnAYVF1-q74.jpg deleted\n",
      "VDg_U-n3t-I.jpg deleted\n",
      "X82cgnMGeD8.jpg deleted\n",
      "XO6KolPTH8U.jpg deleted\n",
      "XY6Iw4kTOEI.jpg deleted\n",
      "yQKNzY4HGGg.jpg deleted\n",
      "ZBbw3WfcxN8.jpg deleted\n"
     ]
    }
   ],
   "source": [
    "# Filter Images\n",
    "files = [f for f in os.listdir(dirpath) if os.path.isfile(f\"{dirpath}/{f}\") and f.endswith(\".jpg\")]\n",
    "image_ids = []\n",
    "i = 0\n",
    "for f in files:\n",
    "\tim = None\n",
    "\ttry:\n",
    "\t\tim = Image.open(f\"{dirpath}/{f}\")\n",
    "\n",
    "\t\tif im.size != (120, 90):\n",
    "\t\t\tim.close()\n",
    "\t\t\tPath.unlink(f\"{dirpath}/{f}\")\n",
    "\t\t\tprint(f\"{f} deleted\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tim.close()\n",
    "\t\tim = None\n",
    "\n",
    "\t\t# Save valid indexes for filtering\n",
    "\t\timage_ids.append(f[:-4])\n",
    "\texcept:\n",
    "\t\t# Close bad files\n",
    "\t\tif im is not None:\n",
    "\t\t\tim.close()\n",
    "\t\t\tim = None\n",
    "\n",
    "\t\t# Delete Bad Files\n",
    "\t\tPath.unlink(f\"{dirpath}/{f}\")\n",
    "\t\tprint(f\"{f} deleted\")\n",
    "\n",
    "\ti += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing\n",
    "This process involves text processing and image processing. This will involve text standardization and vectorization. For the image, it needs to be processed and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thumbnail-w</th>\n",
       "      <th>thumbnail-h</th>\n",
       "      <th>view-count</th>\n",
       "      <th>like-count</th>\n",
       "      <th>comment-count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35734.0</td>\n",
       "      <td>35734.0</td>\n",
       "      <td>3.527600e+04</td>\n",
       "      <td>3.428900e+04</td>\n",
       "      <td>35006.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>9.287259e+04</td>\n",
       "      <td>2.304083e+03</td>\n",
       "      <td>115.992858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.687911e+06</td>\n",
       "      <td>3.117386e+04</td>\n",
       "      <td>1640.940831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3.000000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.815000e+02</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3.153250e+03</td>\n",
       "      <td>7.500000e+01</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.232996e+08</td>\n",
       "      <td>2.686147e+06</td>\n",
       "      <td>146332.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       thumbnail-w  thumbnail-h    view-count    like-count  comment-count\n",
       "count      35734.0      35734.0  3.527600e+04  3.428900e+04   35006.000000\n",
       "mean         120.0         90.0  9.287259e+04  2.304083e+03     115.992858\n",
       "std            0.0          0.0  1.687911e+06  3.117386e+04    1640.940831\n",
       "min          120.0         90.0  0.000000e+00  0.000000e+00       0.000000\n",
       "25%          120.0         90.0  3.000000e+01  1.000000e+00       0.000000\n",
       "50%          120.0         90.0  2.815000e+02  8.000000e+00       1.000000\n",
       "75%          120.0         90.0  3.153250e+03  7.500000e+01      12.000000\n",
       "max          120.0         90.0  2.232996e+08  2.686147e+06  146332.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(datafile, index_col=\"yt-id\")\n",
    "\n",
    "# Filter raw data for thumbnail only entries\n",
    "thumbnail_ids = np.array([f[:-4] for f in os.listdir(dirpath) if os.path.isfile(f\"{dirpath}/{f}\") and f.endswith(\".jpg\")], dtype=str)\n",
    "raw_data = raw_data.loc[thumbnail_ids]\n",
    "\n",
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextVectorization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Tokenize \u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# TODO figure this out\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTextVectorization\u001b[49m(num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, oov_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<OOV>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39madapt(raw_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     14\u001b[0m sequences \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(raw_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TextVectorization' is not defined"
     ]
    }
   ],
   "source": [
    "# Text Processing\n",
    "def text_standardization(raw_strs):\n",
    "\tt = tf.strings.lower(raw_strs)\n",
    "\tt = tf.strings.regex_replace(t, emoji_re, \"\")\n",
    "\tt = tf.strings.regex_replace(t, sw_re, \"\")\n",
    "\tt = tf.strings.regex_replace(t, punc_re, \"\")\n",
    "\tt = tf.strings.regex_replace(t, space_re, \" \")\n",
    "\treturn t\n",
    "\n",
    "# Tokenize \n",
    "# TODO figure this out\n",
    "tokenizer = TextVectorization(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.adapt(raw_data[\"title\"])\n",
    "sequences = tokenizer.texts_to_sequences(raw_data[\"title\"])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=10, padding='post')\n",
    "\n",
    "# Input \n",
    "input_texts = tokenizer(list(raw_data[\"title\"]), padding=True, truncation=True, max_length=10, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    35734.000000\n",
       "mean         0.251066\n",
       "std          0.141486\n",
       "min          0.000000\n",
       "25%          0.142091\n",
       "50%          0.237930\n",
       "75%          0.341665\n",
       "max          0.819749\n",
       "Name: view-count, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Processing\n",
    "scores = raw_data[\"view-count\"] # Grab View Count\n",
    "scores = scores.fillna(0.0) # Replace NaN with 0\n",
    "scores = scores.map(lambda x : np.log10(x + 1)) # Log everything to make it less extreme\n",
    "scores = scores.div(np.log10(MAX_VIEWS + 1)) # Normalized (+1 to prevent one)\n",
    "\n",
    "scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    35734.000000\n",
       "mean         0.158281\n",
       "std          0.365009\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          1.000000\n",
       "Name: view-count, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Boolean Label\n",
    "b_scores = scores.map(lambda x : int(x >= THRESHOLD))\n",
    "\n",
    "b_scores.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly, the Sequential API is used to train a model. However, due to the need for more than one input, the Functional API must be employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"img_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 90, 120, 3)]      0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 90, 120, 32)       2432      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 45, 60, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 45, 60, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 22, 30, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 22, 30, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 11, 15, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 21120)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               2703488   \n",
      "                                                                 \n",
      " dropout_75 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2806593 (10.71 MB)\n",
      "Trainable params: 2806593 (10.71 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Image Portion\n",
    "img_input = Input((90, 120, 3))\n",
    "x = Conv2D(32, 5, activation='relu', padding='same')(img_input)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x_out = Dropout(0.5)(x)\n",
    "x_out = Dense(64, activation='relu')(x_out)\n",
    "img_output = Dense(1, activation='sigmoid')(x_out)\n",
    "\n",
    "img_model = Model(inputs=img_input, outputs=img_output, name=\"img_model\")\n",
    "\n",
    "img_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " tf_bert_model_1 (TFBertMod  TFBaseModelOutputWithPooli   1094822   ['input_3[0][0]',             \n",
      " el)                         ngAndCrossAttentions(last_   40         'input_4[0][0]']             \n",
      "                             hidden_state=(None, 10, 76                                           \n",
      "                             8),                                                                  \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  (None, 768)                  0         ['tf_bert_model_1[0][0]']     \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 128)                  98432     ['tf.__operators__.getitem[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout_76 (Dropout)        (None, 128)                  0         ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 1)                    129       ['dropout_76[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109580801 (418.02 MB)\n",
      "Trainable params: 109580801 (418.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Text Portion\n",
    "text_input = Input(shape=(10,), dtype=tf.int32)\n",
    "attention_mask = Input(shape=(10,), dtype=tf.int32)\n",
    "\n",
    "transformer_output = transformer_model(text_input, attention_mask=attention_mask)\n",
    "y = transformer_output.last_hidden_state[:, 0, :]\n",
    "\n",
    "y = Dense(128, activation='relu')(y)\n",
    "y_out = Dropout(0.5)(y)\n",
    "text_output = Dense(1, activation='sigmoid')(y_out)\n",
    "\n",
    "text_model = Model(inputs=[text_input, attention_mask], outputs=text_output, name=\"text_model\")\n",
    "\n",
    "text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"unitied_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 90, 120, 3)]         0         []                            \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 90, 120, 32)          2432      ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 45, 60, 32)           0         ['conv2d_3[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 45, 60, 64)           18496     ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPoolin  (None, 22, 30, 64)           0         ['conv2d_4[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 22, 30, 128)          73856     ['max_pooling2d_4[0][0]']     \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPoolin  (None, 11, 15, 128)          0         ['conv2d_5[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " tf_bert_model_1 (TFBertMod  TFBaseModelOutputWithPooli   1094822   ['input_3[0][0]',             \n",
      " el)                         ngAndCrossAttentions(last_   40         'input_4[0][0]']             \n",
      "                             hidden_state=(None, 10, 76                                           \n",
      "                             8),                                                                  \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 21120)                0         ['max_pooling2d_5[0][0]']     \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  (None, 768)                  0         ['tf_bert_model_1[0][0]']     \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 128)                  2703488   ['flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 128)                  98432     ['tf.__operators__.getitem[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 256)                  0         ['dense_3[0][0]',             \n",
      " )                                                                   'dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_77 (Dropout)        (None, 256)                  0         ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 64)                   16448     ['dropout_77[0][0]']          \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 1)                    65        ['dense_12[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 112395457 (428.75 MB)\n",
      "Trainable params: 112395457 (428.75 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# United Model\n",
    "z = Concatenate()([x, y])\n",
    "z = Dropout(0.5)(z)\n",
    "z = Dense(64, activation='relu')(z)\n",
    "z = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "united_model = Model(inputs=[img_input, text_input, attention_mask], outputs=z, name=\"unitied_model\")\n",
    "\n",
    "united_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Using k-fold cross validation, we can judge the accuarcy of this model. To start, we need to make a generator class to batch the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThumbnailDataGenerator(Sequence):\n",
    "\n",
    "\tdef __init__(self, filedir : str, list_IDs : list[str], labels : dict[str, float], rescale : float=255.0, filetype : str=\"jpg\", batch_size : int=32, dim : tuple[int, int]=(90, 120), shuffle=True, **kwargs):\n",
    "\t\t'''\n",
    "\t\tData Generator Initialization Function \n",
    "\t\t'''\n",
    "\t\tself.filedir = filedir\n",
    "\t\tself.filetype = filetype\n",
    "\t\tself.dim = dim\n",
    "\t\tself.rescale = rescale\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.labels = labels\n",
    "\t\tself.list_IDs = list_IDs\n",
    "\t\tself.shuffle = shuffle\n",
    "\t\tself.on_epoch_end()\n",
    "\n",
    "\tdef on_epoch_end(self):\n",
    "\t\t'''\n",
    "\t\tUpdates indexes after each epoch\n",
    "\t\t'''\n",
    "\t\tself.indexes = np.arange(len(self.list_IDs))\n",
    "\n",
    "\t\t# Randomize if Shuffle\n",
    "\t\tif self.shuffle:\n",
    "\t\t\tnp.random.shuffle(self.indexes)\n",
    "\n",
    "\tdef __data_generation(self, list_IDs_temp):\n",
    "\t\t'''\n",
    "\t\tGenerates data containing batch_size samples\n",
    "\t\t'''\n",
    "\t\t# Initialization\n",
    "\t\tX = np.empty((self.batch_size, *self.dim, 3))\n",
    "\t\ty = np.empty((self.batch_size), dtype=float)\n",
    "\n",
    "\t\t# Generate data\n",
    "\t\tfor i, ID in enumerate(list_IDs_temp):\n",
    "\t\t\t# Store sample\n",
    "\t\t\tX[i,] = Image.open(f'{self.filedir}/{ID}.{self.filetype}')\n",
    "\n",
    "\t\t\t# Store class\n",
    "\t\t\ty[i] = self.labels[ID]\n",
    "\n",
    "\t\t# Rescale\n",
    "\t\tX /= self.rescale\n",
    "\n",
    "\t\treturn X, y\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\t'''\n",
    "\t\tDenotes the number of batches per epoch\n",
    "\t\t'''\n",
    "\t\treturn int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\t'''\n",
    "\t\tGenerate one batch of data\n",
    "\t\t'''\n",
    "\t\t# Generate indexes of the batch\n",
    "\t\tindexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "\t\t# Find list of IDs\n",
    "\t\tlist_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "\t\t# Generate data\n",
    "\t\treturn self.__data_generation(list_IDs_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image-only model will undergo a k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting k-Fold #1\n",
      "Epoch 1/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3641 - accuracy: 0.8474 - precision: 0.6194 - recall: 0.1732\n",
      "Epoch 1: val_accuracy improved from -inf to 0.85882, saving model to models\\model_1.keras\n",
      "952/952 [==============================] - 97s 101ms/step - loss: 0.3641 - accuracy: 0.8474 - precision: 0.6194 - recall: 0.1732 - val_loss: 0.3856 - val_accuracy: 0.8588 - val_precision: 0.4752 - val_recall: 0.2735\n",
      "Epoch 2/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3543 - accuracy: 0.8518 - precision: 0.6369 - recall: 0.2185\n",
      "Epoch 2: val_accuracy improved from 0.85882 to 0.86905, saving model to models\\model_1.keras\n",
      "952/952 [==============================] - 97s 102ms/step - loss: 0.3543 - accuracy: 0.8518 - precision: 0.6369 - recall: 0.2185 - val_loss: 0.3366 - val_accuracy: 0.8690 - val_precision: 0.6169 - val_recall: 0.1263\n",
      "Epoch 3/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3402 - accuracy: 0.8592 - precision: 0.6712 - recall: 0.2727\n",
      "Epoch 3: val_accuracy did not improve from 0.86905\n",
      "952/952 [==============================] - 96s 101ms/step - loss: 0.3402 - accuracy: 0.8592 - precision: 0.6712 - recall: 0.2727 - val_loss: 0.3461 - val_accuracy: 0.8655 - val_precision: 0.5267 - val_recall: 0.2210\n",
      "Epoch 4/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3206 - accuracy: 0.8683 - precision: 0.6939 - recall: 0.3490\n",
      "Epoch 4: val_accuracy did not improve from 0.86905\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.3206 - accuracy: 0.8683 - precision: 0.6939 - recall: 0.3490 - val_loss: 0.3522 - val_accuracy: 0.8620 - val_precision: 0.4957 - val_recall: 0.2365\n",
      "Epoch 5/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2954 - accuracy: 0.8807 - precision: 0.7297 - recall: 0.4300\n",
      "Epoch 5: val_accuracy did not improve from 0.86905\n",
      "952/952 [==============================] - 100s 105ms/step - loss: 0.2954 - accuracy: 0.8807 - precision: 0.7297 - recall: 0.4300 - val_loss: 0.3667 - val_accuracy: 0.8625 - val_precision: 0.4989 - val_recall: 0.2324\n",
      "Epoch 6/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2748 - accuracy: 0.8893 - precision: 0.7468 - recall: 0.4882\n",
      "Epoch 6: val_accuracy did not improve from 0.86905\n",
      "952/952 [==============================] - 95s 100ms/step - loss: 0.2748 - accuracy: 0.8893 - precision: 0.7468 - recall: 0.4882 - val_loss: 0.3699 - val_accuracy: 0.8553 - val_precision: 0.4622 - val_recall: 0.3177\n",
      "Epoch 7/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2439 - accuracy: 0.9029 - precision: 0.7774 - recall: 0.5691\n",
      "Epoch 7: val_accuracy did not improve from 0.86905\n",
      "952/952 [==============================] - 97s 102ms/step - loss: 0.2439 - accuracy: 0.9029 - precision: 0.7774 - recall: 0.5691 - val_loss: 0.3814 - val_accuracy: 0.8553 - val_precision: 0.4583 - val_recall: 0.2969\n",
      "Epoch 8/10\n",
      "375/952 [==========>...................] - ETA: 54s - loss: 0.2104 - accuracy: 0.9176 - precision: 0.8131 - recall: 0.6290"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m tbdg_validate \u001b[38;5;241m=\u001b[39m ThumbnailDataGenerator(dirpath, thumbnail_ids[val], b_scores\u001b[38;5;241m.\u001b[39miloc[val], batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Fit\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mimg_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtbdg_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtbdg_validate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Grab Results\u001b[39;00m\n\u001b[0;32m     27\u001b[0m img_model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodeldir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_var\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py:1804\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1796\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1798\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1801\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1802\u001b[0m ):\n\u001b[0;32m   1803\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1804\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1806\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Image Model\n",
    "kf = KFold(n_folds)\n",
    "\n",
    "validation_accuracy = []\n",
    "validation_loss = []\n",
    "\n",
    "fold_var = 1\n",
    "for train, val in kf.split(thumbnail_ids, b_scores):\n",
    "\t# Fold Indicator\n",
    "\tprint(f\"Starting k-Fold #{fold_var}\")\n",
    "\n",
    "\t# Make image model for testing\n",
    "\timg_model = Model(inputs=img_input, outputs=img_output, name=\"img_model\")\n",
    "\timg_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall'])\n",
    "\n",
    "\t# Callback Saving\n",
    "\tcheckpoint = ModelCheckpoint(f\"{modeldir}/model_{fold_var}.keras\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\t# Generator\n",
    "\ttbdg_train = ThumbnailDataGenerator(dirpath, thumbnail_ids[train], b_scores.iloc[train], batch_size=batch_size)\n",
    "\ttbdg_validate = ThumbnailDataGenerator(dirpath, thumbnail_ids[val], b_scores.iloc[val], batch_size=batch_size)\n",
    "\n",
    "\t# Fit\n",
    "\thistory = img_model.fit(x=tbdg_train, validation_data=tbdg_validate, callbacks=[checkpoint], epochs=epochs)\n",
    "\n",
    "\t# Grab Results\n",
    "\timg_model.load_weights(f\"{modeldir}/model_{fold_var}.h5\")\n",
    "\t\n",
    "\tresults = img_model.evaluate(x=tbdg_validate)\n",
    "\tresults = dict(zip(img_model.metrics_names, results))\n",
    "\t\n",
    "\tvalidation_accuracy.append(results['accuracy'])\n",
    "\tvalidation_loss.append(results['loss'])\n",
    "\t\n",
    "\t# Clear\n",
    "\tclear_session()\n",
    "\n",
    "\t# Increment\n",
    "\tfold_var += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text only model will undergo the same test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3, 35734]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m validation_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m fold_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_scores\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m\t\u001b[49m\u001b[38;5;66;43;03m# Fold Indicator\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m\t\u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mStarting k-Fold #\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfold_var\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m\t\u001b[49m\u001b[38;5;66;43;03m# Make image model for testing\u001b[39;49;00m\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:406\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m        The testing set indices for that split.\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m     X, y, groups \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:514\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 514\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3, 35734]"
     ]
    }
   ],
   "source": [
    "# Image Model\n",
    "kf = KFold(n_folds)\n",
    "\n",
    "validation_accuracy = []\n",
    "validation_loss = []\n",
    "\n",
    "fold_var = 1\n",
    "for train, val in kf.split(input_texts, b_scores):\n",
    "\t# Fold Indicator\n",
    "\tprint(f\"Starting k-Fold #{fold_var}\")\n",
    "\n",
    "\t# Make image model for testing\n",
    "\ttext_model = Model(inputs=[text_input, attention_mask], outputs=text_output, name=\"text_model\")\n",
    "\ttext_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall'])\n",
    "\n",
    "\t# Callback Saving\n",
    "\tcheckpoint = ModelCheckpoint(f\"{modeldir}/model_{fold_var}.keras\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\t# Fit\n",
    "\thistory = text_model.fit(x=input_texts[train], y=b_scores.iloc[train], validation_data=(input_texts[val], b_scores.iloc[val]), callbacks=[checkpoint], epochs=epochs)\n",
    "\n",
    "\t# Grab Results\n",
    "\ttext_model.load_weights(f\"{modeldir}/model_{fold_var}.h5\")\n",
    "\t\n",
    "\tresults = text_model.evaluate(x=input_texts[val], y=b_scores.iloc[val])\n",
    "\tresults = dict(zip(text_model.metrics_names, results))\n",
    "\t\n",
    "\tvalidation_accuracy.append(results['accuracy'])\n",
    "\tvalidation_loss.append(results['loss'])\n",
    "\t\n",
    "\t# Clear\n",
    "\tclear_session()\n",
    "\n",
    "\t# Increment\n",
    "\tfold_var += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
