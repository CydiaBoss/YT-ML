{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Machine Learning Model\n",
    "#### Created by Randhir and Andrew\n",
    "\n",
    "Model that will take a $90\\times120$ thumbnail JPEG and title from YouTube to output a video performance metric.\n",
    "The metric will be \n",
    "$$Score=\\log{(View\\ Count + 1)}$$\n",
    "The idea is that the video that attracted more views is a good video. The value is log-scaled as the higher the view count, the less meaningful it becomes. This value will be normalized with the maximum value in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports\n",
    "This cell contains all the necessary imports of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import string, requests, os, json, random\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "from pathlib import Path\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_keras.backend import clear_session\n",
    "from tf_keras.callbacks import ModelCheckpoint\n",
    "from tf_keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Concatenate, Input\n",
    "from tf_keras.preprocessing.text import Tokenizer\n",
    "from tf_keras.preprocessing.sequence import pad_sequences\n",
    "from tf_keras.utils import Sequence\n",
    "from tf_keras import Model\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file with your api key\n",
    "if not load_dotenv():\n",
    "\tprint(\".env file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants\n",
    "This cell contains the constants used by this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# File Structure\n",
    "dirpath = \"thumbnail\"\n",
    "modeldir = \"models\"\n",
    "datafile = \"data-filtered.csv\"\n",
    "\n",
    "# Data Aquisition\n",
    "filepath = \"data.csv\"\n",
    "count = 50\n",
    "max_iterations = 100 # 50 * 100 = 5000 videos\n",
    "topic_id = \"/m/03hf_rm\" # Strategy Games\n",
    "lang = \"en\"\n",
    "API_KEY = os.getenv(\"APIKEY\")\n",
    "\n",
    "# Data Filtering\n",
    "MULT_CSV = False\n",
    "filepath = \"data.csv\"\n",
    "filepath_2 = \"data_2.csv\"\n",
    "filepath_final = \"data-filtered.csv\"\n",
    "lang = \"en\"\n",
    "\n",
    "# Labeling\n",
    "MAX_VIEWS = 15.3e9 # Baby Shark Video (Most Viewed Video)\n",
    "THRESHOLD = 0.4\n",
    "\n",
    "# Vectorization\n",
    "vectorizator_model = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(vectorizator_model)\n",
    "transformer_model = TFBertModel.from_pretrained(vectorizator_model)\n",
    "\n",
    "# Text Model Settings\n",
    "text_input_dim = 20000\n",
    "embedding_dim = 128\n",
    "sequence_length = 500\n",
    "\n",
    "# KFold Settings\n",
    "n_folds = 5\n",
    "epochs = 5\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Aquisition\n",
    "The YouTube API is used to get video data. This includes a video's thumbnail and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data file already exist\n",
    "if os.path.isfile(filepath):\n",
    "    df = pd.read_csv(filepath, index_col=\"yt-id\")\n",
    "else:\n",
    "    df = pd.DataFrame([], columns=[\"yt-id\", \"title\", \"created\", \"channel-id\", \"thumbnail\", \"thumbnail-w\", \"thumbnail-h\", \"view-count\", \"like-count\", \"comment-count\", \"query\"])\n",
    "    df = df.set_index(\"yt-id\")\n",
    "    \n",
    "# Grab missing data IDs for query\n",
    "yt_ids = list(df[df[\"view-count\"].isna()].index)\n",
    "\n",
    "# Loop\n",
    "yt_reads = 0\n",
    "for i in range(max_iterations):\n",
    "    try:\n",
    "        # Check if any stats calls are needed\n",
    "        if len(yt_ids) > 0:\n",
    "            # Message \n",
    "            print(\"Pulling statistics for missing data values\")\n",
    "\n",
    "            # Split up batch by 50 if needed\n",
    "            for index_split in range(50, len(yt_ids) + 1, 50):\n",
    "                # Generate & call statistic query (1 unit)\n",
    "                urlData_stats = f\"https://www.googleapis.com/youtube/v3/videos?key={API_KEY}&part=statistics&id={','.join(yt_ids[index_split - 50:index_split])}\"\n",
    "                webURL_stats = urllib.request.urlopen(urlData_stats)\n",
    "                raw_stats_data = webURL_stats.read()\n",
    "                results_stats = json.loads(raw_stats_data.decode(webURL_stats.info().get_content_charset('utf-8')))\n",
    "\n",
    "                # Process Stats Response\n",
    "                for stats_data in results_stats[\"items\"]:\n",
    "                    try:\n",
    "                        # Parse data\n",
    "                        new_row = pd.DataFrame([{\n",
    "                            \"yt-id\": stats_data['id'],\n",
    "                            \"view-count\": stats_data['statistics']['viewCount'],\n",
    "                            \"like-count\": stats_data['statistics']['likeCount'] if 'likeCount' in stats_data['statistics'] else \"\",\n",
    "                            \"comment-count\": stats_data['statistics']['commentCount'] if 'commentCount' in stats_data['statistics'] else \"\",\n",
    "                        },])\n",
    "                        new_row = new_row.set_index(\"yt-id\")\n",
    "\n",
    "                        # Update main dataset\n",
    "                        df.update(new_row)\n",
    "                    except KeyError:\n",
    "                        # Weird Entry\n",
    "                        continue\n",
    "\n",
    "            # Reset after used\n",
    "            yt_ids = [] \n",
    "\n",
    "            # Message \n",
    "            print(\"Finished pulling statistics for current batch\")\n",
    "\n",
    "        # Message\n",
    "        print(f\"Pulling {count} random videos\")\n",
    "\n",
    "        # Generates random query for YT\n",
    "        r_q = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(3))\n",
    "\n",
    "        # Calls the API for search results (100 units)\n",
    "        urlData_query = f\"https://www.googleapis.com/youtube/v3/search?key={API_KEY}&maxResults={count}&part=snippet&type=video&relevanceLanguage={lang}&topicId={topic_id}&q={r_q}\"\n",
    "        webURL_query = urllib.request.urlopen(urlData_query)\n",
    "        raw_vid_data = webURL_query.read()\n",
    "        results_vids = json.loads(raw_vid_data.decode(webURL_query.info().get_content_charset('utf-8')))\n",
    "\n",
    "        # Process Video Response\n",
    "        for video_data in results_vids['items']:\n",
    "            # Ignore Live and Upcoming Content (no ratings yet)\n",
    "            if video_data['snippet']['liveBroadcastContent'] != \"none\":\n",
    "                continue\n",
    "\n",
    "            # Parse data\n",
    "            try:\n",
    "                new_row = pd.DataFrame([{\n",
    "                    \"yt-id\": video_data['id']['videoId'],\n",
    "                    \"title\": video_data['snippet']['title'],\n",
    "                    \"created\": video_data['snippet']['publishedAt'],\n",
    "                    \"channel-id\": video_data['snippet']['channelId'],\n",
    "                    \"thumbnail\": video_data['snippet']['thumbnails'][\"default\"][\"url\"],\n",
    "                    \"thumbnail-w\": video_data['snippet']['thumbnails'][\"default\"][\"width\"],\n",
    "                    \"thumbnail-h\": video_data['snippet']['thumbnails'][\"default\"][\"height\"],\n",
    "                    \"query\": r_q,\n",
    "                },])\n",
    "                new_row = new_row.set_index(\"yt-id\")\n",
    "\n",
    "                try:\n",
    "                    # Append\n",
    "                    df = pd.concat([df, new_row], verify_integrity=True)\n",
    "\n",
    "                    # Store your ids\n",
    "                    yt_reads += 1\n",
    "\n",
    "                    # Prepare id for stats query\n",
    "                    yt_ids.append(video_data['id']['videoId'])\n",
    "                except ValueError:\n",
    "                    # Duplicate video detected\n",
    "                    continue\n",
    "            except KeyError:\n",
    "                # Weird Entry\n",
    "                continue\n",
    "\n",
    "        # Update User\n",
    "        print(f\"API call #{i + 1} successfully\")\n",
    "\n",
    "        # Dumb Data to prevent loss every 5 runs\n",
    "        if i % 5 == 0:\n",
    "            df.to_csv(filepath)\n",
    "\n",
    "    # ON API failure, quit and save\n",
    "    except urllib.error.HTTPError:\n",
    "        print(\"Latest API call failed. You are likely out of units. Try again tomorrow.\")\n",
    "        break\n",
    "    \n",
    "# Write to csv\n",
    "df.to_csv(filepath)\n",
    "\n",
    "# Termination\n",
    "print(f\"Was able to pull {yt_reads} rows\")\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After aquiring the data, the thumbnail images need to be pulled as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Filtering\n",
    "Some of the pulled data need to be filtered before usage. This includes potential duplicates and non english entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "df = pd.read_csv(filepath, index_col=\"yt-id\")\n",
    "\n",
    "# Merge multiple if needed\n",
    "if MULT_CSV:\n",
    "\tdf_2 = pd.read_csv(filepath_2, index_col=\"yt-id\")\n",
    "\tdf = pd.concat([df, df_2])\n",
    "\n",
    "print(f\"{df.size} rows in data file\")\n",
    "\n",
    "# Remove duplicates\n",
    "df = df[~df.index.duplicated(keep='first')]\n",
    "print(f\"{df.size} rows remaining after duplication filter\")\n",
    "\n",
    "# Remove non language\n",
    "def lang_filter(row) -> bool:\n",
    "\ttry:\n",
    "\t\tprint(row[\"title\"])\n",
    "\t\treturn detect(row[\"title\"]) == lang\n",
    "\texcept LangDetectException:\n",
    "\t\treturn False\n",
    "\t\n",
    "df = df[df.apply(lang_filter, axis=1)]\n",
    "print(f\"{df.size} rows remaining after translation filter\")\n",
    "\n",
    "# Save Filtered Data\n",
    "df.to_csv(filepath_final)\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thumbnail Requesting\n",
    "Once the dataset has been filtered, the thumbnails can now be pulled. Images that do not fit the $90\\times120$ size will be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab data\n",
    "df = pd.read_csv(filepath, index_col=\"yt-id\")\n",
    "\n",
    "# Make directory for image if not already\n",
    "if not os.path.isdir(dirpath):\n",
    "\tos.mkdir(dirpath)\n",
    "\n",
    "# Iterate thru dataframe and download\n",
    "def grab_thumbnail(x : pd.Series):\n",
    "\t# Check if file exist\n",
    "\tfilename = f'{dirpath}/{x.name}.jpg'\n",
    "\tif os.path.isfile(filename):\n",
    "\t\tprint(f\"Thumbnail already retrieved for {x.name}\")\n",
    "\t\treturn\n",
    "\n",
    "\t# Call file\n",
    "\twith open(filename, 'wb') as handle:\n",
    "\t\tprint(f\"Retrieving thumbnail for {x.name}\")\n",
    "\t\tresponse = requests.get(x[\"thumbnail\"], stream=True)\n",
    "\n",
    "\t\t# Fail request\n",
    "\t\tif not response.ok:\n",
    "\t\t\tprint(f\"Could not retrieve thumbnail for {x.name}\")\n",
    "\n",
    "\t\t# Success save\n",
    "\t\tfor block in response.iter_content(1024):\n",
    "\t\t\tif not block:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\thandle.write(block)\n",
    "\n",
    "# Apply to all\n",
    "df.apply(grab_thumbnail, axis=1)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02v-CVttnS0.jpg deleted\n",
      "0EZUP5Vtemw.jpg deleted\n",
      "4KlB4i4dEWU.jpg deleted\n",
      "6JhUQpe-J6U.jpg deleted\n",
      "bAHQy0QFUMI.jpg deleted\n",
      "DcejDtVA4MU.jpg deleted\n",
      "E0Hchyxwr4c.jpg deleted\n",
      "ffLdLgSbpEc.jpg deleted\n",
      "hstJLLvhYSM.jpg deleted\n",
      "htE2M7shdfI.jpg deleted\n",
      "JTwsU2dDpEg.jpg deleted\n",
      "Lpnw6hMIu24.jpg deleted\n",
      "Q9D-aQzRuU4.jpg deleted\n",
      "RPoQZ_926hQ.jpg deleted\n",
      "Sl2ueV8kRRU.jpg deleted\n",
      "StkNJFSGksg.jpg deleted\n",
      "tnAYVF1-q74.jpg deleted\n",
      "VDg_U-n3t-I.jpg deleted\n",
      "X82cgnMGeD8.jpg deleted\n",
      "XO6KolPTH8U.jpg deleted\n",
      "XY6Iw4kTOEI.jpg deleted\n",
      "yQKNzY4HGGg.jpg deleted\n",
      "ZBbw3WfcxN8.jpg deleted\n"
     ]
    }
   ],
   "source": [
    "# Filter Images\n",
    "files = [f for f in os.listdir(dirpath) if os.path.isfile(f\"{dirpath}/{f}\") and f.endswith(\".jpg\")]\n",
    "image_ids = []\n",
    "i = 0\n",
    "for f in files:\n",
    "\tim = None\n",
    "\ttry:\n",
    "\t\tim = Image.open(f\"{dirpath}/{f}\")\n",
    "\n",
    "\t\tif im.size != (120, 90):\n",
    "\t\t\tim.close()\n",
    "\t\t\tPath.unlink(f\"{dirpath}/{f}\")\n",
    "\t\t\tprint(f\"{f} deleted\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tim.close()\n",
    "\t\tim = None\n",
    "\n",
    "\t\t# Save valid indexes for filtering\n",
    "\t\timage_ids.append(f[:-4])\n",
    "\texcept:\n",
    "\t\t# Close bad files\n",
    "\t\tif im is not None:\n",
    "\t\t\tim.close()\n",
    "\t\t\tim = None\n",
    "\n",
    "\t\t# Delete Bad Files\n",
    "\t\tPath.unlink(f\"{dirpath}/{f}\")\n",
    "\t\tprint(f\"{f} deleted\")\n",
    "\n",
    "\ti += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing\n",
    "This process involves text processing and image processing. This will involve text standardization and vectorization. For the image, it needs to be processed and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thumbnail-w</th>\n",
       "      <th>thumbnail-h</th>\n",
       "      <th>view-count</th>\n",
       "      <th>like-count</th>\n",
       "      <th>comment-count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35734.0</td>\n",
       "      <td>35734.0</td>\n",
       "      <td>3.527600e+04</td>\n",
       "      <td>3.428900e+04</td>\n",
       "      <td>35006.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>9.287259e+04</td>\n",
       "      <td>2.304083e+03</td>\n",
       "      <td>115.992858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.687911e+06</td>\n",
       "      <td>3.117386e+04</td>\n",
       "      <td>1640.940831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3.000000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.815000e+02</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3.153250e+03</td>\n",
       "      <td>7.500000e+01</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.232996e+08</td>\n",
       "      <td>2.686147e+06</td>\n",
       "      <td>146332.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       thumbnail-w  thumbnail-h    view-count    like-count  comment-count\n",
       "count      35734.0      35734.0  3.527600e+04  3.428900e+04   35006.000000\n",
       "mean         120.0         90.0  9.287259e+04  2.304083e+03     115.992858\n",
       "std            0.0          0.0  1.687911e+06  3.117386e+04    1640.940831\n",
       "min          120.0         90.0  0.000000e+00  0.000000e+00       0.000000\n",
       "25%          120.0         90.0  3.000000e+01  1.000000e+00       0.000000\n",
       "50%          120.0         90.0  2.815000e+02  8.000000e+00       1.000000\n",
       "75%          120.0         90.0  3.153250e+03  7.500000e+01      12.000000\n",
       "max          120.0         90.0  2.232996e+08  2.686147e+06  146332.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(datafile, index_col=\"yt-id\")\n",
    "\n",
    "# Filter raw data for thumbnail only entries\n",
    "thumbnail_ids = pd.Series([f[:-4] for f in os.listdir(dirpath) if os.path.isfile(f\"{dirpath}/{f}\") and f.endswith(\".jpg\")], dtype=str)\n",
    "raw_data = raw_data.loc[thumbnail_ids]\n",
    "\n",
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer_layer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer_layer.fit_on_texts(raw_data[\"title\"])\n",
    "sequences = tokenizer_layer.texts_to_sequences(raw_data[\"title\"])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=10, padding='post')\n",
    "\n",
    "# Input \n",
    "input_texts = tokenizer(list(raw_data[\"title\"]), padding=True, truncation=True, max_length=10, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    35734.000000\n",
       "mean         0.251066\n",
       "std          0.141486\n",
       "min          0.000000\n",
       "25%          0.142091\n",
       "50%          0.237930\n",
       "75%          0.341665\n",
       "max          0.819749\n",
       "Name: view-count, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Processing\n",
    "scores = raw_data[\"view-count\"] # Grab View Count\n",
    "scores = scores.fillna(0.0) # Replace NaN with 0\n",
    "scores = scores.map(lambda x : np.log10(x + 1)) # Log everything to make it less extreme\n",
    "scores = scores.div(np.log10(MAX_VIEWS + 1)) # Normalized (+1 to prevent one)\n",
    "\n",
    "scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    35734.000000\n",
       "mean         0.158281\n",
       "std          0.365009\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          1.000000\n",
       "Name: view-count, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Boolean Label\n",
    "b_scores = scores.map(lambda x : int(x >= THRESHOLD))\n",
    "\n",
    "b_scores.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly, the Sequential API is used to train a model. However, due to the need for more than one input, the Functional API must be employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"img_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " image_inputs (InputLayer)   [(None, 90, 120, 3)]      0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 90, 120, 32)       2432      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 45, 60, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 45, 60, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 22, 30, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 22, 30, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 11, 15, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 21120)             0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               2703488   \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2806593 (10.71 MB)\n",
      "Trainable params: 2806593 (10.71 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Image Portion\n",
    "img_input = Input((90, 120, 3), name=\"image_inputs\")\n",
    "x = Conv2D(32, 5, activation='relu', padding='same')(img_input)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x_out = Dropout(0.5)(x)\n",
    "x_out = Dense(64, activation='relu')(x_out)\n",
    "img_output = Dense(1, activation='sigmoid')(x_out)\n",
    "\n",
    "img_model = Model(inputs=img_input, outputs=img_output, name=\"img_model\")\n",
    "\n",
    "img_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " text_inputs (InputLayer)    [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " attention_masks (InputLaye  [(None, 10)]                 0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel  TFBaseModelOutputWithPooli   1094822   ['text_inputs[0][0]',         \n",
      " )                           ngAndCrossAttentions(last_   40         'attention_masks[0][0]']     \n",
      "                             hidden_state=(None, 10, 76                                           \n",
      "                             8),                                                                  \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  (None, 768)                  0         ['tf_bert_model[0][0]']       \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 128)                  98432     ['tf.__operators__.getitem[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)        (None, 128)                  0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 1)                    129       ['dropout_38[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109580801 (418.02 MB)\n",
      "Trainable params: 109580801 (418.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Text Portion\n",
    "text_input = Input(shape=(10,), dtype=tf.int32, name=\"text_inputs\")\n",
    "attention_mask = Input(shape=(10,), dtype=tf.int32, name=\"attention_masks\")\n",
    "\n",
    "transformer_output = transformer_model(text_input, attention_mask=attention_mask)\n",
    "y = transformer_output.last_hidden_state[:, 0, :]\n",
    "\n",
    "y = Dense(128, activation='relu')(y)\n",
    "y_out = Dropout(0.5)(y)\n",
    "text_output = Dense(1, activation='sigmoid')(y_out)\n",
    "\n",
    "text_model = Model(inputs=[text_input, attention_mask], outputs=text_output, name=\"text_model\")\n",
    "\n",
    "text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"unitied_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " image_inputs (InputLayer)   [(None, 90, 120, 3)]         0         []                            \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 90, 120, 32)          2432      ['image_inputs[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 45, 60, 32)           0         ['conv2d_3[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 45, 60, 64)           18496     ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPoolin  (None, 22, 30, 64)           0         ['conv2d_4[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 22, 30, 128)          73856     ['max_pooling2d_4[0][0]']     \n",
      "                                                                                                  \n",
      " text_inputs (InputLayer)    [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " attention_masks (InputLaye  [(None, 10)]                 0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPoolin  (None, 11, 15, 128)          0         ['conv2d_5[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel  TFBaseModelOutputWithPooli   1094822   ['text_inputs[0][0]',         \n",
      " )                           ngAndCrossAttentions(last_   40         'attention_masks[0][0]']     \n",
      "                             hidden_state=(None, 10, 76                                           \n",
      "                             8),                                                                  \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 21120)                0         ['max_pooling2d_5[0][0]']     \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  (None, 768)                  0         ['tf_bert_model[0][0]']       \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 128)                  2703488   ['flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 128)                  98432     ['tf.__operators__.getitem[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 256)                  0         ['dense_7[0][0]',             \n",
      " )                                                                   'dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_41 (Dropout)        (None, 256)                  0         ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 64)                   16448     ['dropout_41[0][0]']          \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 1)                    65        ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 112395457 (428.75 MB)\n",
      "Trainable params: 112395457 (428.75 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# United Model\n",
    "z = Concatenate()([x, y])\n",
    "z = Dropout(0.5)(z)\n",
    "z = Dense(64, activation='relu')(z)\n",
    "z = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "united_model = Model(inputs=[img_input, text_input, attention_mask], outputs=z, name=\"unitied_model\")\n",
    "\n",
    "united_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generators\n",
    "To lower memory usage, the dataset needs to be batched. This is done through the use of generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type 'Series' is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mThumbnailDataGenerator\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m\t\u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m\t\t\t  \u001b[49m\u001b[43mfiledir\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m\t\t\t  \u001b[49m\u001b[43mlist_IDs\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[43m\t\t\t  \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250;43m\t\t\u001b[39;49m\u001b[38;5;124;43;03m'''\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;43;03m\t\tData Generator Initialization Function \u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;43;03m\t\t'''\u001b[39;49;00m\n",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m, in \u001b[0;36mThumbnailDataGenerator\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mThumbnailDataGenerator\u001b[39;00m(Sequence):\n\u001b[0;32m      3\u001b[0m \t\u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[0;32m      4\u001b[0m \t\t\t  filedir : \u001b[38;5;28mstr\u001b[39m, \n\u001b[1;32m----> 5\u001b[0m \t\t\t  list_IDs : \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m, \n\u001b[0;32m      6\u001b[0m \t\t\t  labels : pd\u001b[38;5;241m.\u001b[39mSeries[\u001b[38;5;28mint\u001b[39m], \n\u001b[0;32m      7\u001b[0m \t\t\t  rescale : \u001b[38;5;28mfloat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255.0\u001b[39m, \n\u001b[0;32m      8\u001b[0m \t\t\t  filetype : \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      9\u001b[0m \t\t\t  batch_size : \u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, \n\u001b[0;32m     10\u001b[0m \t\t\t  dim : \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m90\u001b[39m, \u001b[38;5;241m120\u001b[39m), \n\u001b[0;32m     11\u001b[0m \t\t\t  shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \t\t):\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m\t\t\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\t\tData Generator Initialization Function \u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\t\t'''\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \t\t\u001b[38;5;66;03m# File Location & Info\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: type 'Series' is not subscriptable"
     ]
    }
   ],
   "source": [
    "class ThumbnailDataGenerator(Sequence):\n",
    "\n",
    "\tdef __init__(self, \n",
    "\t\t\t  filedir : str, \n",
    "\t\t\t  list_IDs : pd.Series, \n",
    "\t\t\t  labels : pd.Series, \n",
    "\t\t\t  rescale : float=255.0, \n",
    "\t\t\t  filetype : str=\"jpg\", \n",
    "\t\t\t  batch_size : int=32, \n",
    "\t\t\t  dim : tuple[int, int]=(90, 120), \n",
    "\t\t\t  shuffle=True\n",
    "\t\t):\n",
    "\t\t'''\n",
    "\t\tData Generator Initialization Function \n",
    "\t\t'''\n",
    "\t\t# File Location & Info\n",
    "\t\tself.filedir = filedir\n",
    "\t\tself.filetype = filetype\n",
    "\n",
    "\t\t# Image Settings\n",
    "\t\tself.dim = dim\n",
    "\t\tself.rescale = rescale\n",
    "\n",
    "\t\t# Batching\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.shuffle = shuffle\n",
    "\n",
    "\t\t# Datasets\n",
    "\t\tself.labels = labels\n",
    "\t\tself.list_IDs = list_IDs\n",
    "\n",
    "\t\tself.on_epoch_end()\n",
    "\n",
    "\tdef on_epoch_end(self):\n",
    "\t\t'''\n",
    "\t\tUpdates indexes after each epoch\n",
    "\t\t'''\n",
    "\t\tself.indexes = np.arange(len(self.list_IDs))\n",
    "\n",
    "\t\t# Randomize if Shuffle\n",
    "\t\tif self.shuffle:\n",
    "\t\t\tnp.random.shuffle(self.indexes)\n",
    "\n",
    "\tdef __data_generation(self, list_IDs_temp):\n",
    "\t\t'''\n",
    "\t\tGenerates data containing batch_size samples\n",
    "\t\t'''\n",
    "\t\t# Initialization\n",
    "\t\tX = np.empty((self.batch_size, *self.dim, 3))\n",
    "\t\ty = np.empty((self.batch_size), dtype=float)\n",
    "\n",
    "\t\t# Generate data\n",
    "\t\tfor i, ID in enumerate(list_IDs_temp):\n",
    "\t\t\t# Store sample\n",
    "\t\t\tX[i,] = Image.open(f'{self.filedir}/{ID}.{self.filetype}')\n",
    "\n",
    "\t\t\t# Store class\n",
    "\t\t\ty[i] = self.labels[ID]\n",
    "\n",
    "\t\t# Rescale\n",
    "\t\tX /= self.rescale\n",
    "\n",
    "\t\treturn X, y\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\t'''\n",
    "\t\tDenotes the number of batches per epoch\n",
    "\t\t'''\n",
    "\t\treturn int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\t'''\n",
    "\t\tGenerate one batch of data\n",
    "\t\t'''\n",
    "\t\t# Generate indexes of the batch\n",
    "\t\tindexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "\t\t# Find list of IDs\n",
    "\t\tlist_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "\t\t# Generate data\n",
    "\t\treturn self.__data_generation(list_IDs_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThumbnailTitleDataGenerator(Sequence):\n",
    "\n",
    "\tdef __init__(self, \n",
    "\t\t\t  filedir : str, \n",
    "\t\t\t  list_IDs : pd.Series[str], \n",
    "\t\t\t  texts : pd.Series[str], \n",
    "\t\t\t  labels : pd.Series[float], \n",
    "\t\t\t  rescale : float=255.0, \n",
    "\t\t\t  filetype : str=\"jpg\", \n",
    "\t\t\t  batch_size : int=32, \n",
    "\t\t\t  img_dim : tuple[int, int]=(90, 120), \n",
    "\t\t\t  text_dim : int=10, \n",
    "\t\t\t  tokenizer : BertTokenizer=None,\n",
    "\t\t\t  img_input_name : str=\"image_inputs\",\n",
    "\t\t\t  text_input_name : str=\"text_inputs\",\n",
    "\t\t\t  attention_input_name : str=\"attention_masks\",\n",
    "\t\t\t  shuffle=True\n",
    "\t\t):\n",
    "\t\t'''\n",
    "\t\tData Generator Initialization Function \n",
    "\t\t'''\n",
    "\t\t# File Location & Info\n",
    "\t\tself.filedir = filedir\n",
    "\t\tself.filetype = filetype\n",
    "\n",
    "\t\t# Dimensions\n",
    "\t\tself.img_dim = img_dim\n",
    "\t\tself.text_dim = text_dim\n",
    "\t\tself.rescale = rescale\n",
    "\n",
    "\t\t# Batching\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.shuffle = shuffle\n",
    "\n",
    "\t\t# Datasets\n",
    "\t\tself.labels = labels\n",
    "\t\tself.list_IDs = list_IDs\n",
    "\t\tself.texts = texts\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\n",
    "\t\t# Input Names\n",
    "\t\tself.img_input_name = img_input_name\n",
    "\t\tself.text_input_name = text_input_name\n",
    "\t\tself.attention_input_name = attention_input_name\n",
    "\n",
    "\t\tself.on_epoch_end()\n",
    "\n",
    "\tdef on_epoch_end(self):\n",
    "\t\t'''\n",
    "\t\tUpdates indexes after each epoch\n",
    "\t\t'''\n",
    "\t\tself.indexes = np.arange(len(self.list_IDs))\n",
    "\n",
    "\t\t# Randomize if Shuffle\n",
    "\t\tif self.shuffle:\n",
    "\t\t\tnp.random.shuffle(self.indexes)\n",
    "\n",
    "\tdef __data_generation(self, list_IDs_temp : list[str]):\n",
    "\t\t'''\n",
    "\t\tGenerates data containing batch_size samples\n",
    "\t\t'''\n",
    "\t\t# Initialization\n",
    "\t\tX_IMG = np.empty((self.batch_size, *self.img_dim, 3))\n",
    "\t\tX_TEXT = np.empty((self.batch_size, self.text_dim))\n",
    "\t\tX_ATTENTION = np.empty((self.batch_size, self.text_dim))\n",
    "\t\ty = np.empty((self.batch_size), dtype=float)\n",
    "\n",
    "\t\t# Generate data\n",
    "\t\ttemp_titles = np.empty((self.batch_size), dtype=str)\n",
    "\t\tfor i, ID in enumerate(list_IDs_temp):\n",
    "\t\t\t# Store sample\n",
    "\t\t\tX_IMG[i,] = Image.open(f'{self.filedir}/{ID}.{self.filetype}')\n",
    "\n",
    "\t\t\t# Process title\n",
    "\t\t\ttemp_titles[i] = self.texts[ID]\n",
    "\n",
    "\t\t\t# Store class\n",
    "\t\t\ty[i] = self.labels[ID]\n",
    "\n",
    "\t\t# Rescale\n",
    "\t\tX_IMG /= self.rescale\n",
    "\n",
    "\t\t# Encode Text\n",
    "\t\tencoded_text = self.tokenizer(temp_titles, padding=True, truncation=True, max_length=self.text_dim)\n",
    "\t\tX_TEXT[:,:] = encoded_text[\"input_ids\"]\n",
    "\t\tX_ATTENTION[:,:] = encoded_text[\"attention_masks\"]\n",
    "\n",
    "\t\t# Unify Inputs\n",
    "\t\tX = {\n",
    "\t\t\tself.img_input_name: X_IMG,\n",
    "\t\t\tself.text_input_name: X_TEXT,\n",
    "\t\t\tself.attention_input_name: X_ATTENTION\n",
    "\t\t}\n",
    "\n",
    "\t\treturn X, y\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\t'''\n",
    "\t\tDenotes the number of batches per epoch\n",
    "\t\t'''\n",
    "\t\treturn int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\t'''\n",
    "\t\tGenerate one batch of data\n",
    "\t\t'''\n",
    "\t\t# Generate indexes of the batch\n",
    "\t\tindexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "\t\t# Find list of IDs\n",
    "\t\tlist_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "\t\t# Generate data\n",
    "\t\treturn self.__data_generation(list_IDs_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Using k-fold cross validation, we can judge the accuarcy of this model. The image-only model will undergo a k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting k-Fold #1\n",
      "Epoch 1/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2164 - accuracy: 0.9153 - precision: 0.8144 - recall: 0.6247\n",
      "Epoch 1: val_accuracy improved from -inf to 0.85532, saving model to models\\model_1.keras\n",
      "952/952 [==============================] - 261s 273ms/step - loss: 0.2164 - accuracy: 0.9153 - precision: 0.8144 - recall: 0.6247 - val_loss: 0.4142 - val_accuracy: 0.8553 - val_precision: 0.4575 - val_recall: 0.2908\n",
      "Epoch 2/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1887 - accuracy: 0.9246 - precision: 0.8171 - recall: 0.6939\n",
      "Epoch 2: val_accuracy did not improve from 0.85532\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.1887 - accuracy: 0.9246 - precision: 0.8171 - recall: 0.6939 - val_loss: 0.4615 - val_accuracy: 0.8521 - val_precision: 0.4412 - val_recall: 0.2908\n",
      "Epoch 3/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.9335 - precision: 0.8378 - recall: 0.7351\n",
      "Epoch 3: val_accuracy did not improve from 0.85532\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.1711 - accuracy: 0.9335 - precision: 0.8378 - recall: 0.7351 - val_loss: 0.4848 - val_accuracy: 0.8388 - val_precision: 0.4066 - val_recall: 0.3772\n",
      "Epoch 4/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 0.9377 - precision: 0.8406 - recall: 0.7641\n",
      "Epoch 4: val_accuracy did not improve from 0.85532\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.1571 - accuracy: 0.9377 - precision: 0.8406 - recall: 0.7641 - val_loss: 0.5164 - val_accuracy: 0.8472 - val_precision: 0.4173 - val_recall: 0.2857\n",
      "Epoch 5/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1402 - accuracy: 0.9457 - precision: 0.8612 - recall: 0.7959\n",
      "Epoch 5: val_accuracy did not improve from 0.85532\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.1402 - accuracy: 0.9457 - precision: 0.8612 - recall: 0.7959 - val_loss: 0.5207 - val_accuracy: 0.8340 - val_precision: 0.3764 - val_recall: 0.3147\n",
      "Epoch 6/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1296 - accuracy: 0.9503 - precision: 0.8703 - recall: 0.8178\n",
      "Epoch 6: val_accuracy did not improve from 0.85532\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.1296 - accuracy: 0.9503 - precision: 0.8703 - recall: 0.8178 - val_loss: 0.5782 - val_accuracy: 0.8436 - val_precision: 0.4068 - val_recall: 0.3075\n",
      "Epoch 7/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1198 - accuracy: 0.9534 - precision: 0.8708 - recall: 0.8396\n",
      "Epoch 7: val_accuracy did not improve from 0.85532\n",
      "952/952 [==============================] - 97s 102ms/step - loss: 0.1198 - accuracy: 0.9534 - precision: 0.8708 - recall: 0.8396 - val_loss: 0.6704 - val_accuracy: 0.8518 - val_precision: 0.4412 - val_recall: 0.2946\n",
      "Epoch 8/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1093 - accuracy: 0.9585 - precision: 0.8845 - recall: 0.8587\n",
      "Epoch 8: val_accuracy did not improve from 0.85532\n",
      "952/952 [==============================] - 99s 104ms/step - loss: 0.1093 - accuracy: 0.9585 - precision: 0.8845 - recall: 0.8587 - val_loss: 0.6285 - val_accuracy: 0.8415 - val_precision: 0.4128 - val_recall: 0.3673\n",
      "Epoch 9/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1054 - accuracy: 0.9598 - precision: 0.8878 - recall: 0.8631\n",
      "Epoch 9: val_accuracy did not improve from 0.85532\n",
      "952/952 [==============================] - 99s 104ms/step - loss: 0.1054 - accuracy: 0.9598 - precision: 0.8878 - recall: 0.8631 - val_loss: 0.6708 - val_accuracy: 0.8499 - val_precision: 0.4328 - val_recall: 0.2953\n",
      "Epoch 10/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.0982 - accuracy: 0.9618 - precision: 0.8935 - recall: 0.8700\n",
      "Epoch 10: val_accuracy did not improve from 0.85532\n",
      "952/952 [==============================] - 97s 102ms/step - loss: 0.0982 - accuracy: 0.9618 - precision: 0.8935 - recall: 0.8700 - val_loss: 0.7037 - val_accuracy: 0.8438 - val_precision: 0.4031 - val_recall: 0.2914\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.3515 - accuracy: 0.8686 - precision: 0.5599 - recall: 0.2049\n",
      "Starting k-Fold #2\n",
      "Epoch 1/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3457 - accuracy: 0.8557 - precision: 0.6234 - recall: 0.1857\n",
      "Epoch 1: val_accuracy improved from -inf to 0.85980, saving model to models\\model_2.keras\n",
      "952/952 [==============================] - 175s 182ms/step - loss: 0.3457 - accuracy: 0.8557 - precision: 0.6234 - recall: 0.1857 - val_loss: 0.3351 - val_accuracy: 0.8598 - val_precision: 0.7104 - val_recall: 0.2835\n",
      "Epoch 2/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3312 - accuracy: 0.8610 - precision: 0.6579 - recall: 0.2256\n",
      "Epoch 2: val_accuracy improved from 0.85980 to 0.85994, saving model to models\\model_2.keras\n",
      "952/952 [==============================] - 177s 186ms/step - loss: 0.3312 - accuracy: 0.8610 - precision: 0.6579 - recall: 0.2256 - val_loss: 0.3325 - val_accuracy: 0.8599 - val_precision: 0.7251 - val_recall: 0.2718\n",
      "Epoch 3/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3155 - accuracy: 0.8668 - precision: 0.6500 - recall: 0.3145\n",
      "Epoch 3: val_accuracy did not improve from 0.85994\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.3155 - accuracy: 0.8668 - precision: 0.6500 - recall: 0.3145 - val_loss: 0.3352 - val_accuracy: 0.8576 - val_precision: 0.7527 - val_recall: 0.2303\n",
      "Epoch 4/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2949 - accuracy: 0.8774 - precision: 0.6787 - recall: 0.4041\n",
      "Epoch 4: val_accuracy did not improve from 0.85994\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.2949 - accuracy: 0.8774 - precision: 0.6787 - recall: 0.4041 - val_loss: 0.3470 - val_accuracy: 0.8577 - val_precision: 0.7173 - val_recall: 0.2554\n",
      "Epoch 5/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2734 - accuracy: 0.8847 - precision: 0.6873 - recall: 0.4775\n",
      "Epoch 5: val_accuracy improved from 0.85994 to 0.86289, saving model to models\\model_2.keras\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.2734 - accuracy: 0.8847 - precision: 0.6873 - recall: 0.4775 - val_loss: 0.3350 - val_accuracy: 0.8629 - val_precision: 0.6862 - val_recall: 0.3405\n",
      "Epoch 6/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2528 - accuracy: 0.8953 - precision: 0.7140 - recall: 0.5471\n",
      "Epoch 6: val_accuracy did not improve from 0.86289\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.2528 - accuracy: 0.8953 - precision: 0.7140 - recall: 0.5471 - val_loss: 0.3385 - val_accuracy: 0.8599 - val_precision: 0.6416 - val_recall: 0.3824\n",
      "Epoch 7/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2293 - accuracy: 0.9035 - precision: 0.7274 - recall: 0.6092\n",
      "Epoch 7: val_accuracy did not improve from 0.86289\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.2293 - accuracy: 0.9035 - precision: 0.7274 - recall: 0.6092 - val_loss: 0.3479 - val_accuracy: 0.8510 - val_precision: 0.5812 - val_recall: 0.4110\n",
      "Epoch 8/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2114 - accuracy: 0.9126 - precision: 0.7494 - recall: 0.6592\n",
      "Epoch 8: val_accuracy did not improve from 0.86289\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.2114 - accuracy: 0.9126 - precision: 0.7494 - recall: 0.6592 - val_loss: 0.3844 - val_accuracy: 0.8546 - val_precision: 0.6211 - val_recall: 0.3480\n",
      "Epoch 9/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1970 - accuracy: 0.9183 - precision: 0.7580 - recall: 0.6985\n",
      "Epoch 9: val_accuracy did not improve from 0.86289\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.1970 - accuracy: 0.9183 - precision: 0.7580 - recall: 0.6985 - val_loss: 0.3641 - val_accuracy: 0.8513 - val_precision: 0.5805 - val_recall: 0.4201\n",
      "Epoch 10/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1801 - accuracy: 0.9261 - precision: 0.7831 - recall: 0.7260\n",
      "Epoch 10: val_accuracy did not improve from 0.86289\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.1801 - accuracy: 0.9261 - precision: 0.7831 - recall: 0.7260 - val_loss: 0.3827 - val_accuracy: 0.8476 - val_precision: 0.5553 - val_recall: 0.4608\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.3310 - accuracy: 0.8609 - precision: 0.6923 - recall: 0.3142\n",
      "Starting k-Fold #3\n",
      "Epoch 1/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3105 - accuracy: 0.8714 - precision: 0.6787 - recall: 0.3380\n",
      "Epoch 1: val_accuracy improved from -inf to 0.88543, saving model to models\\model_3.keras\n",
      "952/952 [==============================] - 100s 104ms/step - loss: 0.3105 - accuracy: 0.8714 - precision: 0.6787 - recall: 0.3380 - val_loss: 0.2947 - val_accuracy: 0.8854 - val_precision: 0.7553 - val_recall: 0.4529\n",
      "Epoch 2/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2927 - accuracy: 0.8764 - precision: 0.6775 - recall: 0.4017\n",
      "Epoch 2: val_accuracy did not improve from 0.88543\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.2927 - accuracy: 0.8764 - precision: 0.6775 - recall: 0.4017 - val_loss: 0.2976 - val_accuracy: 0.8739 - val_precision: 0.7789 - val_recall: 0.3314\n",
      "Epoch 3/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2760 - accuracy: 0.8846 - precision: 0.6880 - recall: 0.4786\n",
      "Epoch 3: val_accuracy did not improve from 0.88543\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.2760 - accuracy: 0.8846 - precision: 0.6880 - recall: 0.4786 - val_loss: 0.3008 - val_accuracy: 0.8730 - val_precision: 0.7605 - val_recall: 0.3387\n",
      "Epoch 4/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2564 - accuracy: 0.8919 - precision: 0.7046 - recall: 0.5331\n",
      "Epoch 4: val_accuracy did not improve from 0.88543\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.2564 - accuracy: 0.8919 - precision: 0.7046 - recall: 0.5331 - val_loss: 0.2949 - val_accuracy: 0.8789 - val_precision: 0.6989 - val_recall: 0.4712\n",
      "Epoch 5/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2337 - accuracy: 0.9022 - precision: 0.7259 - recall: 0.6027\n",
      "Epoch 5: val_accuracy did not improve from 0.88543\n",
      "952/952 [==============================] - 98s 102ms/step - loss: 0.2337 - accuracy: 0.9022 - precision: 0.7259 - recall: 0.6027 - val_loss: 0.2968 - val_accuracy: 0.8770 - val_precision: 0.6975 - val_recall: 0.4530\n",
      "Epoch 6/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2176 - accuracy: 0.9098 - precision: 0.7455 - recall: 0.6433\n",
      "Epoch 6: val_accuracy did not improve from 0.88543\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.2176 - accuracy: 0.9098 - precision: 0.7455 - recall: 0.6433 - val_loss: 0.3069 - val_accuracy: 0.8717 - val_precision: 0.6465 - val_recall: 0.4966\n",
      "Epoch 7/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1991 - accuracy: 0.9169 - precision: 0.7596 - recall: 0.6860\n",
      "Epoch 7: val_accuracy did not improve from 0.88543\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.1991 - accuracy: 0.9169 - precision: 0.7596 - recall: 0.6860 - val_loss: 0.3335 - val_accuracy: 0.8741 - val_precision: 0.7228 - val_recall: 0.3883\n",
      "Epoch 8/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1810 - accuracy: 0.9253 - precision: 0.7848 - recall: 0.7207\n",
      "Epoch 8: val_accuracy did not improve from 0.88543\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.1810 - accuracy: 0.9253 - precision: 0.7848 - recall: 0.7207 - val_loss: 0.3291 - val_accuracy: 0.8675 - val_precision: 0.6321 - val_recall: 0.4793\n",
      "Epoch 9/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1733 - accuracy: 0.9285 - precision: 0.7855 - recall: 0.7475\n",
      "Epoch 9: val_accuracy did not improve from 0.88543\n",
      "952/952 [==============================] - 98s 102ms/step - loss: 0.1733 - accuracy: 0.9285 - precision: 0.7855 - recall: 0.7475 - val_loss: 0.3441 - val_accuracy: 0.8650 - val_precision: 0.6450 - val_recall: 0.4117\n",
      "Epoch 10/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1542 - accuracy: 0.9368 - precision: 0.8107 - recall: 0.7777\n",
      "Epoch 10: val_accuracy did not improve from 0.88543\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.1542 - accuracy: 0.9368 - precision: 0.8107 - recall: 0.7777 - val_loss: 0.3826 - val_accuracy: 0.8634 - val_precision: 0.6630 - val_recall: 0.3548\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.2953 - accuracy: 0.8864 - precision: 0.7416 - recall: 0.4827\n",
      "Starting k-Fold #4\n",
      "Epoch 1/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3035 - accuracy: 0.8752 - precision: 0.6739 - recall: 0.3998\n",
      "Epoch 1: val_accuracy improved from -inf to 0.88627, saving model to models\\model_4.keras\n",
      "952/952 [==============================] - 100s 104ms/step - loss: 0.3035 - accuracy: 0.8752 - precision: 0.6739 - recall: 0.3998 - val_loss: 0.2950 - val_accuracy: 0.8863 - val_precision: 0.8082 - val_recall: 0.3926\n",
      "Epoch 2/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2842 - accuracy: 0.8822 - precision: 0.6923 - recall: 0.4522\n",
      "Epoch 2: val_accuracy did not improve from 0.88627\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.2842 - accuracy: 0.8822 - precision: 0.6923 - recall: 0.4522 - val_loss: 0.2830 - val_accuracy: 0.8793 - val_precision: 0.8153 - val_recall: 0.3313\n",
      "Epoch 3/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2667 - accuracy: 0.8898 - precision: 0.7010 - recall: 0.5228\n",
      "Epoch 3: val_accuracy improved from 0.88627 to 0.88950, saving model to models\\model_4.keras\n",
      "952/952 [==============================] - 99s 104ms/step - loss: 0.2667 - accuracy: 0.8898 - precision: 0.7010 - recall: 0.5228 - val_loss: 0.2747 - val_accuracy: 0.8895 - val_precision: 0.7867 - val_recall: 0.4388\n",
      "Epoch 4/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.8994 - precision: 0.7292 - recall: 0.5731\n",
      "Epoch 4: val_accuracy did not improve from 0.88950\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.2459 - accuracy: 0.8994 - precision: 0.7292 - recall: 0.5731 - val_loss: 0.2709 - val_accuracy: 0.8857 - val_precision: 0.7141 - val_recall: 0.4935\n",
      "Epoch 5/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2306 - accuracy: 0.9039 - precision: 0.7307 - recall: 0.6162\n",
      "Epoch 5: val_accuracy did not improve from 0.88950\n",
      "952/952 [==============================] - 99s 104ms/step - loss: 0.2306 - accuracy: 0.9039 - precision: 0.7307 - recall: 0.6162 - val_loss: 0.2792 - val_accuracy: 0.8874 - val_precision: 0.7131 - val_recall: 0.5125\n",
      "Epoch 6/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2089 - accuracy: 0.9134 - precision: 0.7540 - recall: 0.6676\n",
      "Epoch 6: val_accuracy did not improve from 0.88950\n",
      "952/952 [==============================] - 99s 104ms/step - loss: 0.2089 - accuracy: 0.9134 - precision: 0.7540 - recall: 0.6676 - val_loss: 0.2986 - val_accuracy: 0.8700 - val_precision: 0.6189 - val_recall: 0.5186\n",
      "Epoch 7/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1919 - accuracy: 0.9207 - precision: 0.7750 - recall: 0.6981\n",
      "Epoch 7: val_accuracy did not improve from 0.88950\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.1919 - accuracy: 0.9207 - precision: 0.7750 - recall: 0.6981 - val_loss: 0.2931 - val_accuracy: 0.8775 - val_precision: 0.6462 - val_recall: 0.5431\n",
      "Epoch 8/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1763 - accuracy: 0.9286 - precision: 0.7974 - recall: 0.7323\n",
      "Epoch 8: val_accuracy did not improve from 0.88950\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.1763 - accuracy: 0.9286 - precision: 0.7974 - recall: 0.7323 - val_loss: 0.3028 - val_accuracy: 0.8773 - val_precision: 0.6910 - val_recall: 0.4418\n",
      "Epoch 9/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1714 - accuracy: 0.9305 - precision: 0.8028 - recall: 0.7403\n",
      "Epoch 9: val_accuracy did not improve from 0.88950\n",
      "952/952 [==============================] - 99s 104ms/step - loss: 0.1714 - accuracy: 0.9305 - precision: 0.8028 - recall: 0.7403 - val_loss: 0.3308 - val_accuracy: 0.8702 - val_precision: 0.6406 - val_recall: 0.4529\n",
      "Epoch 10/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1537 - accuracy: 0.9369 - precision: 0.8106 - recall: 0.7810\n",
      "Epoch 10: val_accuracy did not improve from 0.88950\n",
      "952/952 [==============================] - 99s 104ms/step - loss: 0.1537 - accuracy: 0.9369 - precision: 0.8106 - recall: 0.7810 - val_loss: 0.3767 - val_accuracy: 0.8693 - val_precision: 0.7049 - val_recall: 0.3342\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.2762 - accuracy: 0.8895 - precision: 0.7789 - recall: 0.4466\n",
      "Starting k-Fold #5\n",
      "Epoch 1/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2969 - accuracy: 0.8767 - precision: 0.6799 - recall: 0.4192\n",
      "Epoch 1: val_accuracy improved from -inf to 0.89790, saving model to models\\model_5.keras\n",
      "952/952 [==============================] - 100s 104ms/step - loss: 0.2969 - accuracy: 0.8767 - precision: 0.6799 - recall: 0.4192 - val_loss: 0.2523 - val_accuracy: 0.8979 - val_precision: 0.8399 - val_recall: 0.4371\n",
      "Epoch 2/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2794 - accuracy: 0.8836 - precision: 0.6909 - recall: 0.4810\n",
      "Epoch 2: val_accuracy did not improve from 0.89790\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.2794 - accuracy: 0.8836 - precision: 0.6909 - recall: 0.4810 - val_loss: 0.2535 - val_accuracy: 0.8947 - val_precision: 0.8443 - val_recall: 0.4087\n",
      "Epoch 3/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2577 - accuracy: 0.8944 - precision: 0.7153 - recall: 0.5527\n",
      "Epoch 3: val_accuracy improved from 0.89790 to 0.89860, saving model to models\\model_5.keras\n",
      "952/952 [==============================] - 100s 105ms/step - loss: 0.2577 - accuracy: 0.8944 - precision: 0.7153 - recall: 0.5527 - val_loss: 0.2653 - val_accuracy: 0.8986 - val_precision: 0.7922 - val_recall: 0.4840\n",
      "Epoch 4/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2384 - accuracy: 0.9014 - precision: 0.7347 - recall: 0.5906\n",
      "Epoch 4: val_accuracy improved from 0.89860 to 0.89930, saving model to models\\model_5.keras\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.2384 - accuracy: 0.9014 - precision: 0.7347 - recall: 0.5906 - val_loss: 0.2493 - val_accuracy: 0.8993 - val_precision: 0.7957 - val_recall: 0.4871\n",
      "Epoch 5/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2214 - accuracy: 0.9082 - precision: 0.7513 - recall: 0.6293\n",
      "Epoch 5: val_accuracy did not improve from 0.89930\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.2214 - accuracy: 0.9082 - precision: 0.7513 - recall: 0.6293 - val_loss: 0.2563 - val_accuracy: 0.8944 - val_precision: 0.7470 - val_recall: 0.5004\n",
      "Epoch 6/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2041 - accuracy: 0.9165 - precision: 0.7716 - recall: 0.6709\n",
      "Epoch 6: val_accuracy did not improve from 0.89930\n",
      "952/952 [==============================] - 101s 106ms/step - loss: 0.2041 - accuracy: 0.9165 - precision: 0.7716 - recall: 0.6709 - val_loss: 0.2692 - val_accuracy: 0.8895 - val_precision: 0.7868 - val_recall: 0.4122\n",
      "Epoch 7/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1873 - accuracy: 0.9235 - precision: 0.7884 - recall: 0.7072\n",
      "Epoch 7: val_accuracy did not improve from 0.89930\n",
      "952/952 [==============================] - 96s 101ms/step - loss: 0.1873 - accuracy: 0.9235 - precision: 0.7884 - recall: 0.7072 - val_loss: 0.2691 - val_accuracy: 0.8947 - val_precision: 0.7246 - val_recall: 0.5368\n",
      "Epoch 8/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1745 - accuracy: 0.9286 - precision: 0.8009 - recall: 0.7311\n",
      "Epoch 8: val_accuracy did not improve from 0.89930\n",
      "952/952 [==============================] - 97s 102ms/step - loss: 0.1745 - accuracy: 0.9286 - precision: 0.8009 - recall: 0.7311 - val_loss: 0.2700 - val_accuracy: 0.8887 - val_precision: 0.6805 - val_recall: 0.5529\n",
      "Epoch 9/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1643 - accuracy: 0.9327 - precision: 0.8043 - recall: 0.7603\n",
      "Epoch 9: val_accuracy did not improve from 0.89930\n",
      "952/952 [==============================] - 98s 103ms/step - loss: 0.1643 - accuracy: 0.9327 - precision: 0.8043 - recall: 0.7603 - val_loss: 0.3016 - val_accuracy: 0.8845 - val_precision: 0.6772 - val_recall: 0.5120\n",
      "Epoch 10/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1540 - accuracy: 0.9366 - precision: 0.8113 - recall: 0.7812\n",
      "Epoch 10: val_accuracy did not improve from 0.89930\n",
      "952/952 [==============================] - 99s 103ms/step - loss: 0.1540 - accuracy: 0.9366 - precision: 0.8113 - recall: 0.7812 - val_loss: 0.3188 - val_accuracy: 0.8833 - val_precision: 0.7088 - val_recall: 0.4428\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.2407 - accuracy: 0.9060 - precision: 0.8081 - recall: 0.5306\n"
     ]
    }
   ],
   "source": [
    "# Image Model\n",
    "kf = KFold(n_folds)\n",
    "\n",
    "validation_accuracy = []\n",
    "validation_loss = []\n",
    "\n",
    "fold_var = 1\n",
    "for train, val in kf.split(thumbnail_ids, b_scores):\n",
    "\t# Fold Indicator\n",
    "\tprint(f\"Starting k-Fold #{fold_var}\")\n",
    "\n",
    "\t# Make image model for testing\n",
    "\timg_model = Model(inputs=img_input, outputs=img_output, name=\"img_model\")\n",
    "\timg_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall'])\n",
    "\n",
    "\t# Callback Saving\n",
    "\tcheckpoint = ModelCheckpoint(f\"{modeldir}/img-model_{fold_var}.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\t# Generator\n",
    "\ttbdg_train = ThumbnailDataGenerator(dirpath, thumbnail_ids[train], b_scores.iloc[train], batch_size=batch_size)\n",
    "\ttbdg_validate = ThumbnailDataGenerator(dirpath, thumbnail_ids[val], b_scores.iloc[val], batch_size=batch_size)\n",
    "\n",
    "\t# Fit\n",
    "\thistory = img_model.fit(x=tbdg_train, validation_data=tbdg_validate, callbacks=[checkpoint], epochs=epochs)\n",
    "\n",
    "\t# Grab Results\n",
    "\timg_model.load_weights(f\"{modeldir}/img-model_{fold_var}.h5\")\n",
    "\t\n",
    "\tresults = img_model.evaluate(x=tbdg_validate)\n",
    "\tresults = dict(zip(img_model.metrics_names, results))\n",
    "\t\n",
    "\tvalidation_accuracy.append(results['accuracy'])\n",
    "\tvalidation_loss.append(results['loss'])\n",
    "\t\n",
    "\t# Clear\n",
    "\tclear_session()\n",
    "\n",
    "\t# Increment\n",
    "\tfold_var += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text only model will undergo the same test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting k-Fold #1\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "894/894 [==============================] - ETA: 0s - loss: 0.4554 - accuracy: 0.8365\n",
      "Epoch 1: val_accuracy improved from -inf to 0.86260, saving model to models\\text-model_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "894/894 [==============================] - 1289s 1s/step - loss: 0.4554 - accuracy: 0.8365 - val_loss: 0.4043 - val_accuracy: 0.8626\n",
      "Epoch 2/5\n",
      "894/894 [==============================] - ETA: 0s - loss: 0.4520 - accuracy: 0.8365\n",
      "Epoch 2: val_accuracy did not improve from 0.86260\n",
      "894/894 [==============================] - 1182s 1s/step - loss: 0.4520 - accuracy: 0.8365 - val_loss: 0.4010 - val_accuracy: 0.8626\n",
      "Epoch 3/5\n",
      "894/894 [==============================] - ETA: 0s - loss: 0.4500 - accuracy: 0.8365\n",
      "Epoch 3: val_accuracy did not improve from 0.86260\n",
      "894/894 [==============================] - 1183s 1s/step - loss: 0.4500 - accuracy: 0.8365 - val_loss: 0.4057 - val_accuracy: 0.8626\n",
      "Epoch 4/5\n",
      "894/894 [==============================] - ETA: 0s - loss: 0.4492 - accuracy: 0.8365\n",
      "Epoch 4: val_accuracy did not improve from 0.86260\n",
      "894/894 [==============================] - 1172s 1s/step - loss: 0.4492 - accuracy: 0.8365 - val_loss: 0.4004 - val_accuracy: 0.8626\n",
      "Epoch 5/5\n",
      "894/894 [==============================] - ETA: 0s - loss: 0.4494 - accuracy: 0.8365\n",
      "Epoch 5: val_accuracy did not improve from 0.86260\n",
      "894/894 [==============================] - 1169s 1s/step - loss: 0.4494 - accuracy: 0.8365 - val_loss: 0.4047 - val_accuracy: 0.8626\n",
      "224/224 [==============================] - 74s 331ms/step - loss: 0.4043 - accuracy: 0.8626\n",
      "Starting k-Fold #2\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "894/894 [==============================] - ETA: 0s - loss: 0.4396 - accuracy: 0.8442\n",
      "Epoch 1: val_accuracy improved from -inf to 0.83168, saving model to models\\text-model_2.h5\n",
      "894/894 [==============================] - 1341s 1s/step - loss: 0.4396 - accuracy: 0.8442 - val_loss: 0.4568 - val_accuracy: 0.8317\n",
      "Epoch 2/5\n",
      "894/894 [==============================] - ETA: 0s - loss: 0.4374 - accuracy: 0.8442\n",
      "Epoch 2: val_accuracy did not improve from 0.83168\n",
      "894/894 [==============================] - 1270s 1s/step - loss: 0.4374 - accuracy: 0.8442 - val_loss: 0.4577 - val_accuracy: 0.8317\n",
      "Epoch 3/5\n",
      "894/894 [==============================] - ETA: 0s - loss: 0.4368 - accuracy: 0.8442\n",
      "Epoch 3: val_accuracy did not improve from 0.83168\n",
      "894/894 [==============================] - 1266s 1s/step - loss: 0.4368 - accuracy: 0.8442 - val_loss: 0.4540 - val_accuracy: 0.8317\n",
      "Epoch 4/5\n",
      "894/894 [==============================] - ETA: 0s - loss: 0.4358 - accuracy: 0.8442\n",
      "Epoch 4: val_accuracy did not improve from 0.83168\n",
      "894/894 [==============================] - 1264s 1s/step - loss: 0.4358 - accuracy: 0.8442 - val_loss: 0.4534 - val_accuracy: 0.8317\n",
      "Epoch 5/5\n",
      "894/894 [==============================] - ETA: 0s - loss: 0.4363 - accuracy: 0.8442\n",
      "Epoch 5: val_accuracy did not improve from 0.83168\n",
      "894/894 [==============================] - 1262s 1s/step - loss: 0.4363 - accuracy: 0.8442 - val_loss: 0.4543 - val_accuracy: 0.8317\n",
      "224/224 [==============================] - 83s 369ms/step - loss: 0.4568 - accuracy: 0.8317\n",
      "Starting k-Fold #3\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "894/894 [==============================] - ETA: 0s - loss: 0.4395 - accuracy: 0.8435\n",
      "Epoch 1: val_accuracy improved from -inf to 0.83448, saving model to models\\text-model_3.h5\n",
      "894/894 [==============================] - 1364s 1s/step - loss: 0.4395 - accuracy: 0.8435 - val_loss: 0.4490 - val_accuracy: 0.8345\n",
      "Epoch 2/5\n",
      "894/894 [==============================] - ETA: 0s - loss: 0.4386 - accuracy: 0.8435\n",
      "Epoch 2: val_accuracy did not improve from 0.83448\n",
      "894/894 [==============================] - 1381s 2s/step - loss: 0.4386 - accuracy: 0.8435 - val_loss: 0.4494 - val_accuracy: 0.8345\n",
      "Epoch 3/5\n",
      "894/894 [==============================] - ETA: 0s - loss: 0.4377 - accuracy: 0.8435"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 30\u001b[0m\n\u001b[0;32m     24\u001b[0m testset \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     25\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: tf\u001b[38;5;241m.\u001b[39mgather(input_texts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], indices\u001b[38;5;241m=\u001b[39mval), \n\u001b[0;32m     26\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_masks\u001b[39m\u001b[38;5;124m\"\u001b[39m: tf\u001b[38;5;241m.\u001b[39mgather(input_texts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m], indices\u001b[38;5;241m=\u001b[39mval)\n\u001b[0;32m     27\u001b[0m }\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Fit\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtext_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtestset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Grab Results\u001b[39;00m\n\u001b[0;32m     33\u001b[0m text_model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodeldir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/text-model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_var\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py:1853\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1838\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1839\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1840\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         pss_evaluation_shards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pss_evaluation_shards,\n\u001b[0;32m   1852\u001b[0m     )\n\u001b[1;32m-> 1853\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1866\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1867\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1868\u001b[0m }\n\u001b[0;32m   1869\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py:2292\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2288\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   2289\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2290\u001b[0m             ):\n\u001b[0;32m   2291\u001b[0m                 callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 2292\u001b[0m                 logs \u001b[38;5;241m=\u001b[39m \u001b[43mtest_function_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2293\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2294\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2295\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2296\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2297\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2299\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2300\u001b[0m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py:4100\u001b[0m, in \u001b[0;36m_TestFunction.run_step\u001b[1;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_or_iterator, data_handler, step, unused_shards):\n\u001b[1;32m-> 4100\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   4102\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32md:\\OneDrive\\OneDrive - University of Waterloo\\School\\Courses\\4A\\MSE 446\\Project\\YT-ML\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Text Model\n",
    "kf = KFold(n_folds)\n",
    "\n",
    "validation_accuracy = []\n",
    "validation_loss = []\n",
    "\n",
    "fold_var = 1\n",
    "for train, val in kf.split(input_texts[\"input_ids\"], b_scores):\n",
    "\t# Fold Indicator\n",
    "\tprint(f\"Starting k-Fold #{fold_var}\")\n",
    "\n",
    "\t# Make image model for testing\n",
    "\ttext_model = Model(inputs=[text_input, attention_mask], outputs=text_output, name=\"text_model\")\n",
    "\ttext_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\t# Callback Saving\n",
    "\tcheckpoint = ModelCheckpoint(f\"{modeldir}/text-model_{fold_var}.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\t# Train / Validation\n",
    "\ttrainset = {\n",
    "\t\t\"text_inputs\": tf.gather(input_texts[\"input_ids\"], indices=train), \n",
    "\t\t\"attention_masks\": tf.gather(input_texts[\"attention_mask\"], indices=train)\n",
    "\t}\n",
    "\ttestset = {\n",
    "\t\t\"text_inputs\": tf.gather(input_texts[\"input_ids\"], indices=val), \n",
    "\t\t\"attention_masks\": tf.gather(input_texts[\"attention_mask\"], indices=val)\n",
    "\t}\n",
    "\n",
    "\t# Fit\n",
    "\thistory = text_model.fit(x=trainset, y=b_scores.iloc[train], validation_data=(testset, b_scores.iloc[val]), callbacks=[checkpoint], epochs=epochs)\n",
    "\n",
    "\t# Grab Results\n",
    "\ttext_model.load_weights(f\"{modeldir}/text-model_{fold_var}.h5\")\n",
    "\t\n",
    "\tresults = text_model.evaluate(x=testset, y=b_scores.iloc[val])\n",
    "\tresults = dict(zip(text_model.metrics_names, results))\n",
    "\t\n",
    "\tvalidation_accuracy.append(results['accuracy'])\n",
    "\tvalidation_loss.append(results['loss'])\n",
    "\t\n",
    "\t# Clear\n",
    "\tclear_session()\n",
    "\n",
    "\t# Increment\n",
    "\tfold_var += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, both models will combine to create a unified system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "Once the k-fold cross validation is complete, the final model can be trained with all the data and can be tested with complete new data from the API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
