{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Machine Learning Model\n",
    "#### Created by Randhir and Andrew\n",
    "\n",
    "Model that will take a $90\\times120$ thumbnail JPEG and title from YouTube to output a video performance metric.\n",
    "The metric will be \n",
    "$$Score=\\log{(View\\ Count + 1)}$$\n",
    "The idea is that the video that attracted more views is a good video. The value is log-scaled as the higher the view count, the less meaningful it becomes. This value will be normalized with the maximum value in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string, re, requests, os, json, random\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "from pathlib import Path\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend import clear_session\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, TextVectorization, Embedding, Dropout, Concatenate, Input\n",
    "from keras import Model\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from datagen import ThumbnailDataGenerator\n",
    "\n",
    "# Load .env file with your api key\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants\n",
    "This cell contains the constants used by this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# File Structure\n",
    "dirpath = \"thumbnail\"\n",
    "modeldir = \"models\"\n",
    "datafile = \"data-filtered.csv\"\n",
    "\n",
    "# Data Aquisition\n",
    "filepath = \"data.csv\"\n",
    "count = 50\n",
    "max_iterations = 100 # 50 * 100 = 5000 videos\n",
    "topic_id = \"/m/03hf_rm\" # Strategy Games\n",
    "lang = \"en\"\n",
    "API_KEY = os.getenv(\"APIKEY\")\n",
    "\n",
    "# Constants\n",
    "MULT_CSV = False\n",
    "filepath = \"data.csv\"\n",
    "filepath_2 = \"data_2.csv\"\n",
    "filepath_final = \"data-filtered.csv\"\n",
    "lang = \"en\"\n",
    "\n",
    "# Labeling\n",
    "MAX_VIEWS = 15.3e9 # Baby Shark Video (Most Viewed Video)\n",
    "THRESHOLD = 0.4\n",
    "\n",
    "# Regex Patterns\n",
    "emoji_re = \"[\\U000000A9-\\U0010ffff]\"\n",
    "punc_re = f\"[{re.escape(string.punctuation)}]\"\n",
    "\n",
    "# Download Stopwords & pattern\n",
    "nltk.download('stopwords')\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "sw_re = f'(?:{\"|\".join([f\"{re.escape(sw)}\" for sw in stopwords_list])})'\n",
    "\n",
    "# Text Model Settings\n",
    "text_input_dim = 20000\n",
    "embedding_dim = 128\n",
    "sequence_length = 500\n",
    "\n",
    "# KFold Settings\n",
    "n_folds = 5\n",
    "epochs = 10\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Aquisition\n",
    "The YouTube API is used to get video data. This includes a video's thumbnail and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data file already exist\n",
    "if os.path.isfile(filepath):\n",
    "    df = pd.read_csv(filepath, index_col=\"yt-id\")\n",
    "else:\n",
    "    df = pd.DataFrame([], columns=[\"yt-id\", \"title\", \"created\", \"channel-id\", \"thumbnail\", \"thumbnail-w\", \"thumbnail-h\", \"view-count\", \"like-count\", \"comment-count\", \"query\"])\n",
    "    df = df.set_index(\"yt-id\")\n",
    "    \n",
    "# Grab missing data IDs for query\n",
    "yt_ids = list(df[df[\"view-count\"].isna()].index)\n",
    "\n",
    "# Loop\n",
    "yt_reads = 0\n",
    "for i in range(max_iterations):\n",
    "    try:\n",
    "        # Check if any stats calls are needed\n",
    "        if len(yt_ids) > 0:\n",
    "            # Message \n",
    "            print(\"Pulling statistics for missing data values\")\n",
    "\n",
    "            # Split up batch by 50 if needed\n",
    "            for index_split in range(50, len(yt_ids) + 1, 50):\n",
    "                # Generate & call statistic query (1 unit)\n",
    "                urlData_stats = f\"https://www.googleapis.com/youtube/v3/videos?key={API_KEY}&part=statistics&id={','.join(yt_ids[index_split - 50:index_split])}\"\n",
    "                webURL_stats = urllib.request.urlopen(urlData_stats)\n",
    "                raw_stats_data = webURL_stats.read()\n",
    "                results_stats = json.loads(raw_stats_data.decode(webURL_stats.info().get_content_charset('utf-8')))\n",
    "\n",
    "                # Process Stats Response\n",
    "                for stats_data in results_stats[\"items\"]:\n",
    "                    try:\n",
    "                        # Parse data\n",
    "                        new_row = pd.DataFrame([{\n",
    "                            \"yt-id\": stats_data['id'],\n",
    "                            \"view-count\": stats_data['statistics']['viewCount'],\n",
    "                            \"like-count\": stats_data['statistics']['likeCount'] if 'likeCount' in stats_data['statistics'] else \"\",\n",
    "                            \"comment-count\": stats_data['statistics']['commentCount'] if 'commentCount' in stats_data['statistics'] else \"\",\n",
    "                        },])\n",
    "                        new_row = new_row.set_index(\"yt-id\")\n",
    "\n",
    "                        # Update main dataset\n",
    "                        df.update(new_row)\n",
    "                    except KeyError:\n",
    "                        # Weird Entry\n",
    "                        continue\n",
    "\n",
    "            # Reset after used\n",
    "            yt_ids = [] \n",
    "\n",
    "            # Message \n",
    "            print(\"Finished pulling statistics for current batch\")\n",
    "\n",
    "        # Message\n",
    "        print(f\"Pulling {count} random videos\")\n",
    "\n",
    "        # Generates random query for YT\n",
    "        r_q = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(3))\n",
    "\n",
    "        # Calls the API for search results (100 units)\n",
    "        urlData_query = f\"https://www.googleapis.com/youtube/v3/search?key={API_KEY}&maxResults={count}&part=snippet&type=video&relevanceLanguage={lang}&topicId={topic_id}&q={r_q}\"\n",
    "        webURL_query = urllib.request.urlopen(urlData_query)\n",
    "        raw_vid_data = webURL_query.read()\n",
    "        results_vids = json.loads(raw_vid_data.decode(webURL_query.info().get_content_charset('utf-8')))\n",
    "\n",
    "        # Process Video Response\n",
    "        for video_data in results_vids['items']:\n",
    "            # Ignore Live and Upcoming Content (no ratings yet)\n",
    "            if video_data['snippet']['liveBroadcastContent'] != \"none\":\n",
    "                continue\n",
    "\n",
    "            # Parse data\n",
    "            try:\n",
    "                new_row = pd.DataFrame([{\n",
    "                    \"yt-id\": video_data['id']['videoId'],\n",
    "                    \"title\": video_data['snippet']['title'],\n",
    "                    \"created\": video_data['snippet']['publishedAt'],\n",
    "                    \"channel-id\": video_data['snippet']['channelId'],\n",
    "                    \"thumbnail\": video_data['snippet']['thumbnails'][\"default\"][\"url\"],\n",
    "                    \"thumbnail-w\": video_data['snippet']['thumbnails'][\"default\"][\"width\"],\n",
    "                    \"thumbnail-h\": video_data['snippet']['thumbnails'][\"default\"][\"height\"],\n",
    "                    \"query\": r_q,\n",
    "                },])\n",
    "                new_row = new_row.set_index(\"yt-id\")\n",
    "\n",
    "                try:\n",
    "                    # Append\n",
    "                    df = pd.concat([df, new_row], verify_integrity=True)\n",
    "\n",
    "                    # Store your ids\n",
    "                    yt_reads += 1\n",
    "\n",
    "                    # Prepare id for stats query\n",
    "                    yt_ids.append(video_data['id']['videoId'])\n",
    "                except ValueError:\n",
    "                    # Duplicate video detected\n",
    "                    continue\n",
    "            except KeyError:\n",
    "                # Weird Entry\n",
    "                continue\n",
    "\n",
    "        # Update User\n",
    "        print(f\"API call #{i + 1} successfully\")\n",
    "\n",
    "        # Dumb Data to prevent loss every 5 runs\n",
    "        if i % 5 == 0:\n",
    "            df.to_csv(filepath)\n",
    "\n",
    "    # ON API failure, quit and save\n",
    "    except urllib.error.HTTPError:\n",
    "        print(\"Latest API call failed. You are likely out of units. Try again tomorrow.\")\n",
    "        break\n",
    "    \n",
    "# Write to csv\n",
    "df.to_csv(filepath)\n",
    "\n",
    "# Termination\n",
    "print(f\"Was able to pull {yt_reads} rows\")\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After aquiring the data, the thumbnail images need to be pulled as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Filtering\n",
    "Some of the pulled data need to be filtered before usage. This includes potential duplicates and non english entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "df = pd.read_csv(filepath, index_col=\"yt-id\")\n",
    "\n",
    "# Merge multiple if needed\n",
    "if MULT_CSV:\n",
    "\tdf_2 = pd.read_csv(filepath_2, index_col=\"yt-id\")\n",
    "\tdf = pd.concat([df, df_2])\n",
    "\n",
    "print(f\"{df.size} rows in data file\")\n",
    "\n",
    "# Remove duplicates\n",
    "df = df[~df.index.duplicated(keep='first')]\n",
    "print(f\"{df.size} rows remaining after duplication filter\")\n",
    "\n",
    "# Remove non language\n",
    "def lang_filter(row) -> bool:\n",
    "\ttry:\n",
    "\t\tprint(row[\"title\"])\n",
    "\t\treturn detect(row[\"title\"]) == lang\n",
    "\texcept LangDetectException:\n",
    "\t\treturn False\n",
    "\t\n",
    "df = df[df.apply(lang_filter, axis=1)]\n",
    "print(f\"{df.size} rows remaining after translation filter\")\n",
    "\n",
    "# Save Filtered Data\n",
    "df.to_csv(filepath_final)\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thumbnail Requesting\n",
    "Once the dataset has been filtered, the thumbnails can now be pulled. Images that do not fit the $90\\times120$ size will be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab data\n",
    "df = pd.read_csv(filepath, index_col=\"yt-id\")\n",
    "\n",
    "# Make directory for image if not already\n",
    "if not os.path.isdir(dirpath):\n",
    "\tos.mkdir(dirpath)\n",
    "\n",
    "# Iterate thru dataframe and download\n",
    "def grab_thumbnail(x : pd.Series):\n",
    "\t# Check if file exist\n",
    "\tfilename = f'{dirpath}/{x.name}.jpg'\n",
    "\tif os.path.isfile(filename):\n",
    "\t\tprint(f\"Thumbnail already retrieved for {x.name}\")\n",
    "\t\treturn\n",
    "\n",
    "\t# Call file\n",
    "\twith open(filename, 'wb') as handle:\n",
    "\t\tprint(f\"Retrieving thumbnail for {x.name}\")\n",
    "\t\tresponse = requests.get(x[\"thumbnail\"], stream=True)\n",
    "\n",
    "\t\t# Fail request\n",
    "\t\tif not response.ok:\n",
    "\t\t\tprint(f\"Could not retrieve thumbnail for {x.name}\")\n",
    "\n",
    "\t\t# Success save\n",
    "\t\tfor block in response.iter_content(1024):\n",
    "\t\t\tif not block:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\thandle.write(block)\n",
    "\n",
    "# Apply to all\n",
    "df.apply(grab_thumbnail, axis=1)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02v-CVttnS0.jpg deleted\n",
      "0EZUP5Vtemw.jpg deleted\n",
      "4KlB4i4dEWU.jpg deleted\n",
      "6JhUQpe-J6U.jpg deleted\n",
      "bAHQy0QFUMI.jpg deleted\n",
      "DcejDtVA4MU.jpg deleted\n",
      "E0Hchyxwr4c.jpg deleted\n",
      "ffLdLgSbpEc.jpg deleted\n",
      "hstJLLvhYSM.jpg deleted\n",
      "htE2M7shdfI.jpg deleted\n",
      "JTwsU2dDpEg.jpg deleted\n",
      "Lpnw6hMIu24.jpg deleted\n",
      "Q9D-aQzRuU4.jpg deleted\n",
      "RPoQZ_926hQ.jpg deleted\n",
      "Sl2ueV8kRRU.jpg deleted\n",
      "StkNJFSGksg.jpg deleted\n",
      "tnAYVF1-q74.jpg deleted\n",
      "VDg_U-n3t-I.jpg deleted\n",
      "X82cgnMGeD8.jpg deleted\n",
      "XO6KolPTH8U.jpg deleted\n",
      "XY6Iw4kTOEI.jpg deleted\n",
      "yQKNzY4HGGg.jpg deleted\n",
      "ZBbw3WfcxN8.jpg deleted\n"
     ]
    }
   ],
   "source": [
    "# Filter Images\n",
    "files = [f for f in os.listdir(dirpath) if os.path.isfile(f\"{dirpath}/{f}\") and f.endswith(\".jpg\")]\n",
    "image_ids = []\n",
    "i = 0\n",
    "for f in files:\n",
    "\tim = None\n",
    "\ttry:\n",
    "\t\tim = Image.open(f\"{dirpath}/{f}\")\n",
    "\n",
    "\t\tif im.size != (120, 90):\n",
    "\t\t\tim.close()\n",
    "\t\t\tPath.unlink(f\"{dirpath}/{f}\")\n",
    "\t\t\tprint(f\"{f} deleted\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tim.close()\n",
    "\t\tim = None\n",
    "\n",
    "\t\t# Save valid indexes for filtering\n",
    "\t\timage_ids.append(f[:-4])\n",
    "\texcept:\n",
    "\t\t# Close bad files\n",
    "\t\tif im is not None:\n",
    "\t\t\tim.close()\n",
    "\t\t\tim = None\n",
    "\n",
    "\t\t# Delete Bad Files\n",
    "\t\tPath.unlink(f\"{dirpath}/{f}\")\n",
    "\t\tprint(f\"{f} deleted\")\n",
    "\n",
    "\ti += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing\n",
    "This process involves text processing and image processing. This will involve text standardization and vectorization. For the image, it needs to be processed and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thumbnail-w</th>\n",
       "      <th>thumbnail-h</th>\n",
       "      <th>view-count</th>\n",
       "      <th>like-count</th>\n",
       "      <th>comment-count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35734.0</td>\n",
       "      <td>35734.0</td>\n",
       "      <td>3.527600e+04</td>\n",
       "      <td>3.428900e+04</td>\n",
       "      <td>35006.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>9.287259e+04</td>\n",
       "      <td>2.304083e+03</td>\n",
       "      <td>115.992858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.687911e+06</td>\n",
       "      <td>3.117386e+04</td>\n",
       "      <td>1640.940831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3.000000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.815000e+02</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3.153250e+03</td>\n",
       "      <td>7.500000e+01</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.232996e+08</td>\n",
       "      <td>2.686147e+06</td>\n",
       "      <td>146332.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       thumbnail-w  thumbnail-h    view-count    like-count  comment-count\n",
       "count      35734.0      35734.0  3.527600e+04  3.428900e+04   35006.000000\n",
       "mean         120.0         90.0  9.287259e+04  2.304083e+03     115.992858\n",
       "std            0.0          0.0  1.687911e+06  3.117386e+04    1640.940831\n",
       "min          120.0         90.0  0.000000e+00  0.000000e+00       0.000000\n",
       "25%          120.0         90.0  3.000000e+01  1.000000e+00       0.000000\n",
       "50%          120.0         90.0  2.815000e+02  8.000000e+00       1.000000\n",
       "75%          120.0         90.0  3.153250e+03  7.500000e+01      12.000000\n",
       "max          120.0         90.0  2.232996e+08  2.686147e+06  146332.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(datafile, index_col=\"yt-id\")\n",
    "\n",
    "# Filter raw data for thumbnail only entries\n",
    "thumbnail_ids = np.array([f[:-4] for f in os.listdir(dirpath) if os.path.isfile(f\"{dirpath}/{f}\") and f.endswith(\".jpg\")], dtype=str)\n",
    "raw_data = raw_data.loc[thumbnail_ids]\n",
    "\n",
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Processing\n",
    "def text_standardization(raw_strs):\n",
    "\tlower = tf.strings.lower(raw_strs)\n",
    "\temojiless = tf.strings.regex_replace(lower, emoji_re, \"\")\n",
    "\tstopwrdless = tf.strings.regex_replace(emojiless, sw_re, \"\")\n",
    "\tpunctuationless = tf.strings.regex_replace(stopwrdless, punc_re, \"\")\n",
    "\treturn punctuationless\n",
    "\n",
    "# Vectorization Layer\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=text_standardization,\n",
    "    max_tokens=text_input_dim,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    35734.000000\n",
       "mean         0.251066\n",
       "std          0.141486\n",
       "min          0.000000\n",
       "25%          0.142091\n",
       "50%          0.237930\n",
       "75%          0.341665\n",
       "max          0.819749\n",
       "Name: view-count, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Processing\n",
    "scores = raw_data[\"view-count\"] # Grab View Count\n",
    "scores = scores.fillna(0.0) # Replace NaN with 0\n",
    "scores = scores.map(lambda x : np.log10(x + 1)) # Log everything to make it less extreme\n",
    "scores = scores.div(np.log10(MAX_VIEWS + 1)) # Normalized (+1 to prevent one)\n",
    "\n",
    "# Boolean Label\n",
    "b_scores = scores.map(lambda x : int(x >= THRESHOLD))\n",
    "\n",
    "scores.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly, the Sequential API is used to train a model. However, due to the need for more than one input, the Functional API must be employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"img_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 90, 120, 3)]      0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 90, 120, 32)       2432      \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 45, 60, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 45, 60, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPooli  (None, 22, 30, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 22, 30, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPooli  (None, 11, 15, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 21120)             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               2703488   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2806593 (10.71 MB)\n",
      "Trainable params: 2806593 (10.71 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Image Portion\n",
    "img_input = Input((90, 120, 3))\n",
    "x = Conv2D(32, 5, activation='relu', padding='same')(img_input)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "img_output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "img_model = Model(inputs=img_input, outputs=img_output, name=\"img_model\")\n",
    "\n",
    "img_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 1, 128)            2560000   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 128)            0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               12900     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2573001 (9.82 MB)\n",
      "Trainable params: 2573001 (9.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Text Portion\n",
    "text_input = Input((1,), dtype=tf.string)\n",
    "y = Embedding(text_input_dim, embedding_dim, input_length=)(text_input)\n",
    "y = Dropout(0.5)(y)\n",
    "y = Flatten()(y)\n",
    "y = Dense(100, activation='relu', kernel_initializer='he_uniform')(y)\n",
    "text_output = Dense(1, activation='sigmoid')(y)\n",
    "\n",
    "text_model = Model(inputs=text_input, outputs=text_output, name=\"text_model\")\n",
    "\n",
    "text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"unitied_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 90, 120, 3)]         0         []                            \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 88, 118, 16)          448       ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 44, 59, 16)           0         ['conv2d_3[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 42, 57, 32)           4640      ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPoolin  (None, 21, 28, 32)           0         ['conv2d_4[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 19, 26, 64)           18496     ['max_pooling2d_4[0][0]']     \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 1, 128)               2560000   ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPoolin  (None, 9, 13, 64)            0         ['conv2d_5[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 1, 128)               0         ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 7488)                 0         ['max_pooling2d_5[0][0]']     \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)         (None, 128)                  0         ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 100)                  748900    ['flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 100)                  12900     ['flatten_2[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 200)                  0         ['dense_2[0][0]',             \n",
      "                                                                     'dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 10)                   2010      ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 1)                    11        ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3347405 (12.77 MB)\n",
      "Trainable params: 3347405 (12.77 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# United Model\n",
    "z = Concatenate()([x, y])\n",
    "z = Dense(10, activation='relu', kernel_initializer='he_uniform')(z)\n",
    "z = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "united_model = Model(inputs=[img_input, text_input], outputs=z, name=\"unitied_model\")\n",
    "\n",
    "united_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Using k-fold cross validation, we can judge the accuarcy of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.4072 - accuracy: 0.8365 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 1: val_accuracy improved from -inf to 0.86246, saving model to models\\model_1.h5\n",
      "952/952 [==============================] - 168s 175ms/step - loss: 0.4072 - accuracy: 0.8365 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.3592 - val_accuracy: 0.8625 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "952/952 [==============================] - ETA: 0s - loss: 0.3952 - accuracy: 0.8366 - precision: 0.5128 - recall: 0.0086\n",
      "Epoch 2: val_accuracy did not improve from 0.86246\n",
      "952/952 [==============================] - 166s 175ms/step - loss: 0.3952 - accuracy: 0.8366 - precision: 0.5128 - recall: 0.0086 - val_loss: 0.3559 - val_accuracy: 0.8625 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3900 - accuracy: 0.8368 - precision: 0.5287 - recall: 0.0197\n",
      "Epoch 3: val_accuracy improved from 0.86246 to 0.86261, saving model to models\\model_1.h5\n",
      "952/952 [==============================] - 163s 171ms/step - loss: 0.3900 - accuracy: 0.8368 - precision: 0.5287 - recall: 0.0197 - val_loss: 0.3557 - val_accuracy: 0.8626 - val_precision: 1.0000 - val_recall: 0.0010\n",
      "Epoch 4/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3841 - accuracy: 0.8362 - precision: 0.4897 - recall: 0.0358\n",
      "Epoch 4: val_accuracy improved from 0.86261 to 0.86597, saving model to models\\model_1.h5\n",
      "952/952 [==============================] - 165s 173ms/step - loss: 0.3841 - accuracy: 0.8362 - precision: 0.4897 - recall: 0.0358 - val_loss: 0.3428 - val_accuracy: 0.8660 - val_precision: 0.6506 - val_recall: 0.0550\n",
      "Epoch 5/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3771 - accuracy: 0.8398 - precision: 0.5760 - recall: 0.0771\n",
      "Epoch 5: val_accuracy did not improve from 0.86597\n",
      "952/952 [==============================] - 162s 170ms/step - loss: 0.3771 - accuracy: 0.8398 - precision: 0.5760 - recall: 0.0771 - val_loss: 0.3613 - val_accuracy: 0.8648 - val_precision: 0.5475 - val_recall: 0.0998\n",
      "Epoch 6/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3697 - accuracy: 0.8411 - precision: 0.5766 - recall: 0.1064\n",
      "Epoch 6: val_accuracy improved from 0.86597 to 0.86625, saving model to models\\model_1.h5\n",
      "952/952 [==============================] - 162s 170ms/step - loss: 0.3697 - accuracy: 0.8411 - precision: 0.5766 - recall: 0.1064 - val_loss: 0.3413 - val_accuracy: 0.8662 - val_precision: 0.5957 - val_recall: 0.0855\n",
      "Epoch 7/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3603 - accuracy: 0.8453 - precision: 0.6054 - recall: 0.1550\n",
      "Epoch 7: val_accuracy improved from 0.86625 to 0.86653, saving model to models\\model_1.h5\n",
      "952/952 [==============================] - 162s 170ms/step - loss: 0.3603 - accuracy: 0.8453 - precision: 0.6054 - recall: 0.1550 - val_loss: 0.3399 - val_accuracy: 0.8665 - val_precision: 0.6306 - val_recall: 0.0713\n",
      "Epoch 8/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3480 - accuracy: 0.8504 - precision: 0.6347 - recall: 0.1998\n",
      "Epoch 8: val_accuracy improved from 0.86653 to 0.86849, saving model to models\\model_1.h5\n",
      "952/952 [==============================] - 164s 172ms/step - loss: 0.3480 - accuracy: 0.8504 - precision: 0.6347 - recall: 0.1998 - val_loss: 0.3517 - val_accuracy: 0.8685 - val_precision: 0.5599 - val_recall: 0.2047\n",
      "Epoch 9/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3337 - accuracy: 0.8557 - precision: 0.6706 - recall: 0.2306\n",
      "Epoch 9: val_accuracy did not improve from 0.86849\n",
      "952/952 [==============================] - 168s 177ms/step - loss: 0.3337 - accuracy: 0.8557 - precision: 0.6706 - recall: 0.2306 - val_loss: 0.3888 - val_accuracy: 0.8440 - val_precision: 0.3994 - val_recall: 0.2668\n",
      "Epoch 10/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3140 - accuracy: 0.8656 - precision: 0.6656 - recall: 0.3576\n",
      "Epoch 10: val_accuracy did not improve from 0.86849\n",
      "952/952 [==============================] - 168s 176ms/step - loss: 0.3140 - accuracy: 0.8656 - precision: 0.6656 - recall: 0.3576 - val_loss: 0.3584 - val_accuracy: 0.8639 - val_precision: 0.5109 - val_recall: 0.2393\n",
      "238/238 [==============================] - 10s 40ms/step - loss: 0.3517 - accuracy: 0.8685 - precision: 0.5599 - recall: 0.2047\n",
      "Epoch 1/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3441 - accuracy: 0.8558 - precision: 0.6176 - recall: 0.1953\n",
      "Epoch 1: val_accuracy improved from -inf to 0.85308, saving model to models\\model_2.h5\n",
      "952/952 [==============================] - 168s 176ms/step - loss: 0.3441 - accuracy: 0.8558 - precision: 0.6176 - recall: 0.1953 - val_loss: 0.3423 - val_accuracy: 0.8531 - val_precision: 0.8000 - val_recall: 0.1697\n",
      "Epoch 2/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3301 - accuracy: 0.8623 - precision: 0.6614 - recall: 0.2371\n",
      "Epoch 2: val_accuracy improved from 0.85308 to 0.85742, saving model to models\\model_2.h5\n",
      "952/952 [==============================] - 163s 171ms/step - loss: 0.3301 - accuracy: 0.8623 - precision: 0.6614 - recall: 0.2371 - val_loss: 0.3303 - val_accuracy: 0.8574 - val_precision: 0.7840 - val_recall: 0.2113\n",
      "Epoch 3/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3126 - accuracy: 0.8681 - precision: 0.6873 - recall: 0.2816\n",
      "Epoch 3: val_accuracy improved from 0.85742 to 0.86106, saving model to models\\model_2.h5\n",
      "952/952 [==============================] - 166s 175ms/step - loss: 0.3126 - accuracy: 0.8681 - precision: 0.6873 - recall: 0.2816 - val_loss: 0.3307 - val_accuracy: 0.8611 - val_precision: 0.6923 - val_recall: 0.3145\n",
      "Epoch 4/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2977 - accuracy: 0.8738 - precision: 0.6700 - recall: 0.3747\n",
      "Epoch 4: val_accuracy did not improve from 0.86106\n",
      "952/952 [==============================] - 165s 173ms/step - loss: 0.2977 - accuracy: 0.8738 - precision: 0.6700 - recall: 0.3747 - val_loss: 0.3410 - val_accuracy: 0.8583 - val_precision: 0.6779 - val_recall: 0.3012\n",
      "Epoch 5/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2760 - accuracy: 0.8849 - precision: 0.6876 - recall: 0.4790\n",
      "Epoch 5: val_accuracy did not improve from 0.86106\n",
      "952/952 [==============================] - 168s 176ms/step - loss: 0.2760 - accuracy: 0.8849 - precision: 0.6876 - recall: 0.4790 - val_loss: 0.3325 - val_accuracy: 0.8587 - val_precision: 0.6353 - val_recall: 0.3769\n",
      "Epoch 6/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2556 - accuracy: 0.8935 - precision: 0.7095 - recall: 0.5352\n",
      "Epoch 6: val_accuracy did not improve from 0.86106\n",
      "952/952 [==============================] - 167s 175ms/step - loss: 0.2556 - accuracy: 0.8935 - precision: 0.7095 - recall: 0.5352 - val_loss: 0.3496 - val_accuracy: 0.8595 - val_precision: 0.6867 - val_recall: 0.3045\n",
      "Epoch 7/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2367 - accuracy: 0.9008 - precision: 0.7258 - recall: 0.5842\n",
      "Epoch 7: val_accuracy did not improve from 0.86106\n",
      "952/952 [==============================] - 168s 176ms/step - loss: 0.2367 - accuracy: 0.9008 - precision: 0.7258 - recall: 0.5842 - val_loss: 0.3623 - val_accuracy: 0.8566 - val_precision: 0.6474 - val_recall: 0.3253\n",
      "Epoch 8/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2161 - accuracy: 0.9095 - precision: 0.7491 - recall: 0.6300\n",
      "Epoch 8: val_accuracy did not improve from 0.86106\n",
      "952/952 [==============================] - 168s 177ms/step - loss: 0.2161 - accuracy: 0.9095 - precision: 0.7491 - recall: 0.6300 - val_loss: 0.3626 - val_accuracy: 0.8532 - val_precision: 0.6091 - val_recall: 0.3577\n",
      "Epoch 9/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2001 - accuracy: 0.9161 - precision: 0.7627 - recall: 0.6696\n",
      "Epoch 9: val_accuracy did not improve from 0.86106\n",
      "952/952 [==============================] - 168s 177ms/step - loss: 0.2001 - accuracy: 0.9161 - precision: 0.7627 - recall: 0.6696 - val_loss: 0.3972 - val_accuracy: 0.8527 - val_precision: 0.6157 - val_recall: 0.3319\n",
      "Epoch 10/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1785 - accuracy: 0.9291 - precision: 0.7985 - recall: 0.7285\n",
      "Epoch 10: val_accuracy did not improve from 0.86106\n",
      "952/952 [==============================] - 167s 175ms/step - loss: 0.1785 - accuracy: 0.9291 - precision: 0.7985 - recall: 0.7285 - val_loss: 0.4147 - val_accuracy: 0.8473 - val_precision: 0.5972 - val_recall: 0.2862\n",
      "238/238 [==============================] - 10s 40ms/step - loss: 0.3307 - accuracy: 0.8611 - precision: 0.6923 - recall: 0.3145\n",
      "Epoch 1/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3115 - accuracy: 0.8697 - precision: 0.6538 - recall: 0.3562\n",
      "Epoch 1: val_accuracy improved from -inf to 0.88669, saving model to models\\model_3.h5\n",
      "952/952 [==============================] - 169s 176ms/step - loss: 0.3115 - accuracy: 0.8697 - precision: 0.6538 - recall: 0.3562 - val_loss: 0.2950 - val_accuracy: 0.8867 - val_precision: 0.7416 - val_recall: 0.4835\n",
      "Epoch 2/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2926 - accuracy: 0.8762 - precision: 0.6620 - recall: 0.4260\n",
      "Epoch 2: val_accuracy did not improve from 0.88669\n",
      "952/952 [==============================] - 165s 174ms/step - loss: 0.2926 - accuracy: 0.8762 - precision: 0.6620 - recall: 0.4260 - val_loss: 0.2953 - val_accuracy: 0.8807 - val_precision: 0.7444 - val_recall: 0.4242\n",
      "Epoch 3/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2714 - accuracy: 0.8850 - precision: 0.6851 - recall: 0.4903\n",
      "Epoch 3: val_accuracy did not improve from 0.88669\n",
      "952/952 [==============================] - 167s 175ms/step - loss: 0.2714 - accuracy: 0.8850 - precision: 0.6851 - recall: 0.4903 - val_loss: 0.2956 - val_accuracy: 0.8782 - val_precision: 0.7782 - val_recall: 0.3683\n",
      "Epoch 4/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.8944 - precision: 0.7130 - recall: 0.5442\n",
      "Epoch 4: val_accuracy did not improve from 0.88669\n",
      "952/952 [==============================] - 175s 184ms/step - loss: 0.2535 - accuracy: 0.8944 - precision: 0.7130 - recall: 0.5442 - val_loss: 0.2995 - val_accuracy: 0.8805 - val_precision: 0.7563 - val_recall: 0.4098\n",
      "Epoch 5/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2337 - accuracy: 0.9012 - precision: 0.7252 - recall: 0.5930\n",
      "Epoch 5: val_accuracy did not improve from 0.88669\n",
      "952/952 [==============================] - 302s 318ms/step - loss: 0.2337 - accuracy: 0.9012 - precision: 0.7252 - recall: 0.5930 - val_loss: 0.3083 - val_accuracy: 0.8758 - val_precision: 0.7065 - val_recall: 0.4259\n",
      "Epoch 6/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2122 - accuracy: 0.9119 - precision: 0.7570 - recall: 0.6433\n",
      "Epoch 6: val_accuracy did not improve from 0.88669\n",
      "952/952 [==============================] - 177s 186ms/step - loss: 0.2122 - accuracy: 0.9119 - precision: 0.7570 - recall: 0.6433 - val_loss: 0.3302 - val_accuracy: 0.8706 - val_precision: 0.6838 - val_recall: 0.4047\n",
      "Epoch 7/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1947 - accuracy: 0.9194 - precision: 0.7709 - recall: 0.6899\n",
      "Epoch 7: val_accuracy did not improve from 0.88669\n",
      "952/952 [==============================] - 174s 183ms/step - loss: 0.1947 - accuracy: 0.9194 - precision: 0.7709 - recall: 0.6899 - val_loss: 0.3122 - val_accuracy: 0.8723 - val_precision: 0.6520 - val_recall: 0.4886\n",
      "Epoch 8/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1806 - accuracy: 0.9263 - precision: 0.7885 - recall: 0.7232\n",
      "Epoch 8: val_accuracy did not improve from 0.88669\n",
      "952/952 [==============================] - 174s 183ms/step - loss: 0.1806 - accuracy: 0.9263 - precision: 0.7885 - recall: 0.7232 - val_loss: 0.3400 - val_accuracy: 0.8681 - val_precision: 0.6750 - val_recall: 0.3903\n",
      "Epoch 9/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1630 - accuracy: 0.9333 - precision: 0.8041 - recall: 0.7586\n",
      "Epoch 9: val_accuracy did not improve from 0.88669\n",
      "952/952 [==============================] - 175s 184ms/step - loss: 0.1630 - accuracy: 0.9333 - precision: 0.8041 - recall: 0.7586 - val_loss: 0.3474 - val_accuracy: 0.8664 - val_precision: 0.6363 - val_recall: 0.4488\n",
      "Epoch 10/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1565 - accuracy: 0.9352 - precision: 0.8053 - recall: 0.7729\n",
      "Epoch 10: val_accuracy did not improve from 0.88669\n",
      "952/952 [==============================] - 177s 186ms/step - loss: 0.1565 - accuracy: 0.9352 - precision: 0.8053 - recall: 0.7729 - val_loss: 0.3642 - val_accuracy: 0.8695 - val_precision: 0.6611 - val_recall: 0.4327\n",
      "238/238 [==============================] - 10s 44ms/step - loss: 0.2950 - accuracy: 0.8867 - precision: 0.7416 - recall: 0.4835\n",
      "Epoch 1/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.3041 - accuracy: 0.8736 - precision: 0.6716 - recall: 0.3842\n",
      "Epoch 1: val_accuracy improved from -inf to 0.88950, saving model to models\\model_4.h5\n",
      "952/952 [==============================] - 176s 184ms/step - loss: 0.3041 - accuracy: 0.8736 - precision: 0.6716 - recall: 0.3842 - val_loss: 0.2762 - val_accuracy: 0.8895 - val_precision: 0.7778 - val_recall: 0.4469\n",
      "Epoch 2/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2846 - accuracy: 0.8836 - precision: 0.6985 - recall: 0.4570\n",
      "Epoch 2: val_accuracy did not improve from 0.88950\n",
      "952/952 [==============================] - 175s 184ms/step - loss: 0.2846 - accuracy: 0.8836 - precision: 0.6985 - recall: 0.4570 - val_loss: 0.2852 - val_accuracy: 0.8756 - val_precision: 0.8501 - val_recall: 0.2839\n",
      "Epoch 3/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2653 - accuracy: 0.8901 - precision: 0.7027 - recall: 0.5225\n",
      "Epoch 3: val_accuracy did not improve from 0.88950\n",
      "952/952 [==============================] - 173s 182ms/step - loss: 0.2653 - accuracy: 0.8901 - precision: 0.7027 - recall: 0.5225 - val_loss: 0.2795 - val_accuracy: 0.8853 - val_precision: 0.7078 - val_recall: 0.4996\n",
      "Epoch 4/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2453 - accuracy: 0.8998 - precision: 0.7276 - recall: 0.5797\n",
      "Epoch 4: val_accuracy did not improve from 0.88950\n",
      "952/952 [==============================] - 176s 185ms/step - loss: 0.2453 - accuracy: 0.8998 - precision: 0.7276 - recall: 0.5797 - val_loss: 0.2783 - val_accuracy: 0.8826 - val_precision: 0.7264 - val_recall: 0.4443\n",
      "Epoch 5/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2248 - accuracy: 0.9079 - precision: 0.7489 - recall: 0.6233\n",
      "Epoch 5: val_accuracy did not improve from 0.88950\n",
      "952/952 [==============================] - 175s 184ms/step - loss: 0.2248 - accuracy: 0.9079 - precision: 0.7489 - recall: 0.6233 - val_loss: 0.2819 - val_accuracy: 0.8864 - val_precision: 0.7613 - val_recall: 0.4374\n",
      "Epoch 6/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2095 - accuracy: 0.9126 - precision: 0.7518 - recall: 0.6634\n",
      "Epoch 6: val_accuracy did not improve from 0.88950\n",
      "952/952 [==============================] - 175s 184ms/step - loss: 0.2095 - accuracy: 0.9126 - precision: 0.7518 - recall: 0.6634 - val_loss: 0.3005 - val_accuracy: 0.8765 - val_precision: 0.7034 - val_recall: 0.4133\n",
      "Epoch 7/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9223 - precision: 0.7797 - recall: 0.7050\n",
      "Epoch 7: val_accuracy did not improve from 0.88950\n",
      "952/952 [==============================] - 173s 182ms/step - loss: 0.1924 - accuracy: 0.9223 - precision: 0.7797 - recall: 0.7050 - val_loss: 0.3036 - val_accuracy: 0.8734 - val_precision: 0.6254 - val_recall: 0.5487\n",
      "Epoch 8/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1817 - accuracy: 0.9267 - precision: 0.7951 - recall: 0.7195\n",
      "Epoch 8: val_accuracy did not improve from 0.88950\n",
      "952/952 [==============================] - 175s 184ms/step - loss: 0.1817 - accuracy: 0.9267 - precision: 0.7951 - recall: 0.7195 - val_loss: 0.3171 - val_accuracy: 0.8780 - val_precision: 0.7143 - val_recall: 0.4142\n",
      "Epoch 9/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1642 - accuracy: 0.9342 - precision: 0.8144 - recall: 0.7533\n",
      "Epoch 9: val_accuracy did not improve from 0.88950\n",
      "952/952 [==============================] - 169s 178ms/step - loss: 0.1642 - accuracy: 0.9342 - precision: 0.8144 - recall: 0.7533 - val_loss: 0.3131 - val_accuracy: 0.8714 - val_precision: 0.6377 - val_recall: 0.4814\n",
      "Epoch 10/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.1513 - accuracy: 0.9380 - precision: 0.8169 - recall: 0.7805\n",
      "Epoch 10: val_accuracy did not improve from 0.88950\n",
      "952/952 [==============================] - 173s 182ms/step - loss: 0.1513 - accuracy: 0.9380 - precision: 0.8169 - recall: 0.7805 - val_loss: 0.3363 - val_accuracy: 0.8717 - val_precision: 0.6609 - val_recall: 0.4305\n",
      "238/238 [==============================] - 10s 41ms/step - loss: 0.2762 - accuracy: 0.8895 - precision: 0.7778 - recall: 0.4469\n",
      "Epoch 1/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2976 - accuracy: 0.8762 - precision: 0.6736 - recall: 0.4242\n",
      "Epoch 1: val_accuracy improved from -inf to 0.90098, saving model to models\\model_5.h5\n",
      "952/952 [==============================] - 264s 276ms/step - loss: 0.2976 - accuracy: 0.8762 - precision: 0.6736 - recall: 0.4242 - val_loss: 0.2549 - val_accuracy: 0.9010 - val_precision: 0.8596 - val_recall: 0.4454\n",
      "Epoch 2/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2800 - accuracy: 0.8821 - precision: 0.6816 - recall: 0.4794\n",
      "Epoch 2: val_accuracy did not improve from 0.90098\n",
      "952/952 [==============================] - 181s 190ms/step - loss: 0.2800 - accuracy: 0.8821 - precision: 0.6816 - recall: 0.4794 - val_loss: 0.2612 - val_accuracy: 0.8926 - val_precision: 0.9128 - val_recall: 0.3531\n",
      "Epoch 3/10\n",
      "952/952 [==============================] - ETA: 0s - loss: 0.2570 - accuracy: 0.8922 - precision: 0.7116 - recall: 0.5371\n",
      "Epoch 3: val_accuracy improved from 0.90098 to 0.90616, saving model to models\\model_5.h5\n",
      "952/952 [==============================] - 172s 180ms/step - loss: 0.2570 - accuracy: 0.8922 - precision: 0.7116 - recall: 0.5371 - val_loss: 0.2405 - val_accuracy: 0.9062 - val_precision: 0.8084 - val_recall: 0.5315\n",
      "Epoch 4/10\n",
      " 69/952 [=>............................] - ETA: 2:49 - loss: 0.2157 - accuracy: 0.9072 - precision: 0.7344 - recall: 0.5803"
     ]
    }
   ],
   "source": [
    "# Image Model\n",
    "kf = KFold(n_folds)\n",
    "\n",
    "validation_accuracy = []\n",
    "validation_loss = []\n",
    "\n",
    "fold_var = 1\n",
    "for train, val in kf.split(thumbnail_ids, b_scores):\n",
    "\t# Fold Indicator\n",
    "\tprint(f\"Starting k-Fold #{fold_var}\")\n",
    "\n",
    "\t# Make image model for testing\n",
    "\timg_model = Model(inputs=img_input, outputs=img_output, name=\"img_model\")\n",
    "\timg_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall'])\n",
    "\n",
    "\t# Callback Saving\n",
    "\tcheckpoint = ModelCheckpoint(f\"{modeldir}/model_{fold_var}.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\t# Generator\n",
    "\ttbdg_train = ThumbnailDataGenerator(dirpath, thumbnail_ids[train], b_scores.iloc[train], batch_size=batch_size)\n",
    "\ttbdg_validate = ThumbnailDataGenerator(dirpath, thumbnail_ids[val], b_scores.iloc[val], batch_size=batch_size)\n",
    "\n",
    "\t# Fit\n",
    "\thistory = img_model.fit(x=tbdg_train, validation_data=tbdg_validate, callbacks=[checkpoint], epochs=epochs)\n",
    "\n",
    "\t# Grab Results\n",
    "\timg_model.load_weights(f\"{modeldir}/model_{fold_var}.h5\")\n",
    "\t\n",
    "\tresults = img_model.evaluate(x=tbdg_validate)\n",
    "\tresults = dict(zip(img_model.metrics_names, results))\n",
    "\t\n",
    "\tvalidation_accuracy.append(results['accuracy'])\n",
    "\tvalidation_loss.append(results['loss'])\n",
    "\t\n",
    "\t# Clear\n",
    "\tclear_session()\n",
    "\n",
    "\t# Increment\n",
    "\tfold_var += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
